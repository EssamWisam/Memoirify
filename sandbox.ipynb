{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers for Tabular Data Representation:\n",
      "A Survey of Models and Applications\n",
      "\n",
      "Gilbert Badaro Mohammed Saeed Paolo Papotti\n",
      "EURECOM, France\n",
      "\n",
      "{gilbert.badaro,mohammed.saeed,paolo.papotti}@eurecom.fr\n",
      "\n",
      "Abstract\n",
      "\n",
      "In the last few years, the natural language pro-\n",
      "cessing community has witnessed advances\n",
      "in neural representations of free texts with\n",
      "transformer-based language models (LMs).\n",
      "Given the importance of knowledge available\n",
      "in tabular data, recent research efforts extend\n",
      "LMs by developing neural representations for\n",
      "structured data. In this article, we present a\n",
      "survey that analyzes these efforts. We first\n",
      "abstract the different systems according to a\n",
      "traditional machine learning pipeline in terms\n",
      "of training data, input representation, model\n",
      "training, and supported downstream tasks. For\n",
      "each aspect, we characterize and compare the\n",
      "proposed solutions. Finally, we discuss future\n",
      "work directions.\n",
      "\n",
      "1\n",
      "\n",
      "Introduction\n",
      "\n",
      "Many researchers are studying how to represent\n",
      "tabular data with neural models for traditional\n",
      "and new natural language processing (NLP) and\n",
      "data management tasks. These models enable ef-\n",
      "fective data-driven systems that go beyond the\n",
      "limits of traditional declarative specifications built\n",
      "around first order logic and SQL. Examples of\n",
      "tasks include answering queries expressed in\n",
      "natural language (Katsogiannis-Meimarakis and\n",
      "Koutrika, 2021; Herzig et al., 2020; Liu et al.,\n",
      "2021a), performing fact-checking (Chen et al.,\n",
      "2020b; Yang and Zhu, 2021; Aly et al., 2021),\n",
      "doing semantic parsing (Yin et al., 2020; Yu et al.,\n",
      "2021), retrieving relevant tables (Pan et al., 2021;\n",
      "Kosti´c et al., 2021; Glass et al., 2021), understand-\n",
      "ing tables (Suhara et al., 2022; Du et al., 2021),\n",
      "and predicting table content (Deng et al., 2020;\n",
      "Iida et al., 2021). Indeed, tabular data contain an\n",
      "extensive amount of knowledge, necessary in a\n",
      "multitude of tasks, such as business (Chabot et al.,\n",
      "2021) and medical operations (Raghupathi and\n",
      "Raghupathi, 2014; Dash et al., 2019), hence, the\n",
      "importance of developing table representations.\n",
      "\n",
      "Given the success of transformers in developing\n",
      "pre-trained language models (LMs) (Devlin et al.,\n",
      "2019; Liu et al., 2019), we focus our analysis\n",
      "on the extension of this architecture for producing\n",
      "representations of tabular data. Such architectures,\n",
      "based on the attention mechanism, have proven to\n",
      "be successful as well on visual (Dosovitskiy et al.,\n",
      "2020; Khan et al., 2021), audio (Gong et al.,\n",
      "2021), and time series data (Cholakov and Kolev,\n",
      "2021). Indeed, pre-trained LMs are very versa-\n",
      "tile, as demonstrated by the large number of\n",
      "tasks that practitioners solve by using these mod-\n",
      "els with fine-tuning, such as improving Arabic\n",
      "opinion and emotion mining (Antoun et al., 2020;\n",
      "Badaro et al., 2014, 2018a,b,c, 2019, 2020). Re-\n",
      "cent work shows that a similar pre-training strat-\n",
      "egy leads to successful results when language\n",
      "models are developed for tabular data.1\n",
      "\n",
      "As depicted in Figure 1, this survey covers both\n",
      "(1) the transformer-based encoder for pre-training\n",
      "neural representations of tabular data and (2) the\n",
      "target models that use the resulting LM to ad-\n",
      "dress downstream tasks. For (1), the training data\n",
      "consist of a large corpus of tables. Once the rep-\n",
      "resentation for this corpus has been learned, it can\n",
      "be used in (2) for a target task on a given (unseen\n",
      "at pre-training) table, such as the population table,\n",
      "along with its context, namely, relevant text infor-\n",
      "mation such as table header and captions. In most\n",
      "cases, the LM obtained from pre-training with\n",
      "large datasets in (1) is used in (2) by fine-tuning\n",
      "the LM with a labeled downstream dataset, for ex-\n",
      "ample, a table with cells annotated as the answer\n",
      "for the question in the context. Extensions on the\n",
      "typical transformer architecture are applied to ac-\n",
      "count for the tabular structure, which is different\n",
      "and richer in some aspects than traditional free text.\n",
      "\n",
      "1We refer to tabular ‘‘language’’ models with a slight\n",
      "abuse of the name, as the LM captures properties and re-\n",
      "lationships of the structured data rather than those of the\n",
      "language.\n",
      "\n",
      "227\n",
      "\n",
      "Transactions of the Association for Computational Linguistics, vol. 11, pp. 227–249, 2023. https://doi.org/10.1162/tacl a 00544\n",
      "Action Editor: Trevor Cohn. Submission batch: 5/2022; Revision batch: 9/2022; Published 3/2023.\n",
      "c(cid:2) 2023 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license.\n",
      "\n",
      "l\n",
      "\n",
      "D\n",
      "o\n",
      "w\n",
      "n\n",
      "o\n",
      "a\n",
      "d\n",
      "e\n",
      "d\n",
      "\n",
      "f\n",
      "r\n",
      "o\n",
      "m\n",
      "h\n",
      "\n",
      "t\n",
      "t\n",
      "\n",
      "p\n",
      "\n",
      ":\n",
      "/\n",
      "/\n",
      "\n",
      "d\n",
      "i\n",
      "r\n",
      "e\n",
      "c\n",
      "t\n",
      ".\n",
      "\n",
      "m\n",
      "\n",
      "i\n",
      "t\n",
      ".\n",
      "\n",
      "e\n",
      "d\n",
      "u\n",
      "\n",
      "/\n",
      "t\n",
      "\n",
      "a\n",
      "c\n",
      "l\n",
      "/\n",
      "\n",
      "l\n",
      "\n",
      "a\n",
      "r\n",
      "t\n",
      "i\n",
      "c\n",
      "e\n",
      "-\n",
      "p\n",
      "d\n",
      "\n",
      "f\n",
      "/\n",
      "\n",
      "d\n",
      "o\n",
      "\n",
      "i\n",
      "/\n",
      "\n",
      ".\n",
      "\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "6\n",
      "2\n",
      "\n",
      "/\n",
      "t\n",
      "\n",
      "l\n",
      "\n",
      "a\n",
      "c\n",
      "_\n",
      "a\n",
      "_\n",
      "0\n",
      "0\n",
      "5\n",
      "4\n",
      "4\n",
      "2\n",
      "0\n",
      "7\n",
      "4\n",
      "8\n",
      "7\n",
      "3\n",
      "\n",
      "/\n",
      "\n",
      "/\n",
      "t\n",
      "\n",
      "l\n",
      "\n",
      "a\n",
      "c\n",
      "_\n",
      "a\n",
      "_\n",
      "0\n",
      "0\n",
      "5\n",
      "4\n",
      "4\n",
      "p\n",
      "d\n",
      "\n",
      ".\n",
      "\n",
      "f\n",
      "\n",
      "b\n",
      "y\n",
      "g\n",
      "u\n",
      "e\n",
      "s\n",
      "t\n",
      "\n",
      "o\n",
      "n\n",
      "2\n",
      "3\n",
      "D\n",
      "e\n",
      "c\n",
      "e\n",
      "m\n",
      "b\n",
      "e\n",
      "r\n",
      "2\n",
      "0\n",
      "2\n",
      "3\n",
      "\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "\fFigure 1: The overall framework for developing and consuming neural representations for tabular data. Wikipedia\n",
      "Tables or WDC Web Table Corpus are typically used in the pre-training step (1). In (2), a table along with\n",
      "its context (header, the additional question and the highlighted answer) are used for a Question Answering\n",
      "downstream task. Both processes combine the serialized table data with natural language text in the context.\n",
      "\n",
      "While all proposed solutions make contribu-\n",
      "tions to the neural representation of tabular data,\n",
      "there is still no systematic study to compare those\n",
      "representations given the different assumptions\n",
      "and target tasks. In this work, we aim to bring\n",
      "clarity in this space and to provide suitable axes\n",
      "for a classification that highlights the main trends\n",
      "and enables future work to clearly position new\n",
      "results. Our contributions are threefold:\n",
      "\n",
      "1. We characterize the tasks of producing and\n",
      "consuming neural representations of tabular\n",
      "data in an abstract machine learning (ML)\n",
      "pipeline. This abstraction relies on five as-\n",
      "pects that are applicable to all proposals and\n",
      "that are agnostic to methods’ assumptions\n",
      "and final application (Section 2).\n",
      "\n",
      "2. We describe and compare the relevant pro-\n",
      "posals according to the characteristics of the\n",
      "datasets (Section 3), the processing of the\n",
      "input (Section 4), and the adaptations of\n",
      "the transformer architecture to handle tabular\n",
      "data (Section 5).\n",
      "\n",
      "3. To show the impact of the proposed solutions,\n",
      "we present six downstream tasks where they\n",
      "have achieved promising results (Section 6)\n",
      "and discuss how tabular language models are\n",
      "used by systems in practice (Section 7).\n",
      "\n",
      "We conclude the survey with a discussion of the\n",
      "limitations of existing works and future research\n",
      "directions (Section 8).\n",
      "\n",
      "Related surveys. Some recent surveys cover the\n",
      "use of deep learning on tabular data (Borisov et al.,\n",
      "2021; Gorishniy et al., 2021; Shwartz-Ziv and\n",
      "Armon, 2021). These surveys are different from\n",
      "ours in multiple aspects. First, they investigate\n",
      "(including non-\n",
      "how deep learning models\n",
      "\n",
      "transformer-based) compare against classical ML\n",
      "models (mainly gradient boosted trees) by pro-\n",
      "viding empirical results with lack of details over\n",
      "the changes to the model, while our work taxono-\n",
      "mizes transformer-based models in depth. Second,\n",
      "other works focus on standard classification and\n",
      "regression tasks where the term ‘‘tabular data’’ is\n",
      "used for labeled training data, representing data\n",
      "points with columns consisting of features. Every\n",
      "row is an input to the model together with a label\n",
      "(e.g., predicting house prices) (Somepalli et al.,\n",
      "2021). In contrast, we do not consider tabular data\n",
      "necessarily as a training dataset with features and\n",
      "labels, but rather as an input containing informa-\n",
      "tion needed for prediction by a target model, such\n",
      "as question answering on tables. For this reason,\n",
      "while they focus on data generation and classifica-\n",
      "tion, we relate design choices and contributions to\n",
      "a large set of downstream tasks. A recent survey\n",
      "(Dong et al., 2022) covers approaches for table\n",
      "pre-training and usage of LMs in downstream\n",
      "tasks with an overlap in terms of surveyed works.\n",
      "In contrast with their work, we provide a more\n",
      "detailed study of the extensions to the transformer\n",
      "architecture and analyze in detail the relevant\n",
      "datasets’ properties and pre-processing steps.\n",
      "\n",
      "2 Terminology and Overview\n",
      "\n",
      "We focus on tabular data that come in the form of\n",
      "a table, as depicted in the two examples in Table 1.\n",
      "Table kinds differ in terms of horizontal/vertical\n",
      "orientation and with respect to the presence of\n",
      "hierarchies. A relational table, which is the most\n",
      "common kind, consists of rows, or records, and\n",
      "columns that together identify cell values, or cells.\n",
      "Columns represent attributes for a given table, and\n",
      "each row represents an instance having those at-\n",
      "tributes. This can be seen as a vertical orientation,\n",
      "\n",
      "228\n",
      "\n",
      "l\n",
      "\n",
      "D\n",
      "o\n",
      "w\n",
      "n\n",
      "o\n",
      "a\n",
      "d\n",
      "e\n",
      "d\n",
      "\n",
      "f\n",
      "r\n",
      "o\n",
      "m\n",
      "h\n",
      "\n",
      "t\n",
      "t\n",
      "\n",
      "p\n",
      "\n",
      ":\n",
      "/\n",
      "/\n",
      "\n",
      "d\n",
      "i\n",
      "r\n",
      "e\n",
      "c\n",
      "t\n",
      ".\n",
      "\n",
      "m\n",
      "\n",
      "i\n",
      "t\n",
      ".\n",
      "\n",
      "e\n",
      "d\n",
      "u\n",
      "\n",
      "/\n",
      "t\n",
      "\n",
      "a\n",
      "c\n",
      "l\n",
      "/\n",
      "\n",
      "l\n",
      "\n",
      "a\n",
      "r\n",
      "t\n",
      "i\n",
      "c\n",
      "e\n",
      "-\n",
      "p\n",
      "d\n",
      "\n",
      "f\n",
      "/\n",
      "\n",
      "d\n",
      "o\n",
      "\n",
      "i\n",
      "/\n",
      "\n",
      ".\n",
      "\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "6\n",
      "2\n",
      "\n",
      "/\n",
      "t\n",
      "\n",
      "l\n",
      "\n",
      "a\n",
      "c\n",
      "_\n",
      "a\n",
      "_\n",
      "0\n",
      "0\n",
      "5\n",
      "4\n",
      "4\n",
      "2\n",
      "0\n",
      "7\n",
      "4\n",
      "8\n",
      "7\n",
      "3\n",
      "\n",
      "/\n",
      "\n",
      "/\n",
      "t\n",
      "\n",
      "l\n",
      "\n",
      "a\n",
      "c\n",
      "_\n",
      "a\n",
      "_\n",
      "0\n",
      "0\n",
      "5\n",
      "4\n",
      "4\n",
      "p\n",
      "d\n",
      "\n",
      ".\n",
      "\n",
      "f\n",
      "\n",
      "b\n",
      "y\n",
      "g\n",
      "u\n",
      "e\n",
      "s\n",
      "t\n",
      "\n",
      "o\n",
      "n\n",
      "2\n",
      "3\n",
      "D\n",
      "e\n",
      "c\n",
      "e\n",
      "m\n",
      "b\n",
      "e\n",
      "r\n",
      "2\n",
      "0\n",
      "2\n",
      "3\n",
      "\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "\fPlayer Team FG%\n",
      "\n",
      "Player Carter\n",
      "\n",
      "Carter\n",
      "Smith\n",
      "\n",
      "LA\n",
      "SF\n",
      "\n",
      "56\n",
      "55\n",
      "\n",
      "Team\n",
      "FG%\n",
      "\n",
      "LA\n",
      "55\n",
      "\n",
      "Table 1: Examples of (vertical) relational table\n",
      "and (horizontal) entity table. A label ‘Identity’\n",
      "on a top row spanning ‘Player’ and ‘Team’\n",
      "would make the relational table a spreadsheet,\n",
      "similarly for the entity table with the same label\n",
      "spanning the first two rows.\n",
      "\n",
      "where all cells in a column share the same atomic\n",
      "type, for example, the relational table in Table 1.\n",
      "An entity table has the same properties, but with\n",
      "a horizontal structure with only one entity whose\n",
      "properties are organized as rows. Spreadsheets,\n",
      "or matrix tables, are the most general kind with\n",
      "information that can be organized both horizon-\n",
      "tally and vertically, possibly with hierarchies in\n",
      "the header and formatting metadata, such as in\n",
      "financial tables.\n",
      "\n",
      "Tables can have rich metadata, such as attribute\n",
      "types (e.g., DATE), domain constraints, functional\n",
      "dependencies across columns and integrity con-\n",
      "straints such as primary keys. In the table sample\n",
      "in Figure 1, column Country is a primary key.\n",
      "Most systems focus on a single table, with or\n",
      "without metadata. However, a few systems, such\n",
      "as GTR (Wang et al., 2021a), GRAPPA (Yu et al.,\n",
      "2021), and DTR (Herzig et al., 2021), consume\n",
      "databases, which are collections of relational ta-\n",
      "bles, possibly under referential constraints. We\n",
      "identify as input the table(s) and its context. The\n",
      "context is a text associated to the table. Depending\n",
      "on the dataset and task at hand, it varies from table\n",
      "metadata, the text surrounding the tables or their\n",
      "captions up to questions, expressed in natural lan-\n",
      "guage, that can be answered with the tabular data\n",
      "(Badaro and Papotti, 2022).\n",
      "\n",
      "The main advantage of the transformer archi-\n",
      "tecture is its ability to generate a LM, a large\n",
      "neural network, with self-supervised pre-training.\n",
      "This pre-trained LM is then usually followed by\n",
      "supervised fine-tuning to adapt it to the target task\n",
      "with a small amount of training data. While trans-\n",
      "formers have proven to be effective in modeling\n",
      "textual content, tables have a rich structure that\n",
      "comes with its own relationships, such as those\n",
      "across values in the rows and attributes. New so-\n",
      "lutions are therefore needed to jointly model the\n",
      "\n",
      "characteristics of the table, its text content, and\n",
      "the text in the table context. As shown in Figure 1,\n",
      "we distinguish two main phases to spell out these\n",
      "contributions. First, we focus on the development\n",
      "of tabular LMs by using transformer-based deep\n",
      "neural networks (1). Given a table and its context,\n",
      "the goal is to learn a pre-trained representation of\n",
      "the structured data (cell values, rows, attributes)\n",
      "in a continuous vector space. We then discuss the\n",
      "use of those representations in the downstream\n",
      "tasks (2).\n",
      "\n",
      "Figure 1 shows the reference pipeline and the\n",
      "aspects that we propose to model existing systems.\n",
      "\n",
      "• Training Datasets (Sec. 3): the datasets used\n",
      "for pre-training and fine-tuning the models\n",
      "toward specific tasks; datasets for the latter\n",
      "case usually come with annotations and/or\n",
      "labels.\n",
      "\n",
      "• Input Processing (Sec. 4): the steps to pre-\n",
      "pare the data for the model processing,\n",
      "such as the transformation from the two di-\n",
      "mensional tabular space to one dimensional\n",
      "input.\n",
      "\n",
      "• Transformer-based Encoder (Sec. 5): the\n",
      "pre-training objectives and customization of\n",
      "the typical transformer-based deep learning\n",
      "architecture.\n",
      "\n",
      "• Downstream Task Model (Sec. 6):\n",
      "\n",
      "the\n",
      "models consuming the representations or\n",
      "fine-tuning them to tackle downstream tasks.\n",
      "\n",
      "• Tabular Language Model (Sec. 7): the out-\n",
      "put representations, including at the token,\n",
      "row, column, table level, and their usage.\n",
      "\n",
      "For the tasks consuming the models, we report\n",
      "on Table based Fact-Checking (TFC), Question\n",
      "Answering (QA), Semantic Parsing (SP), Table\n",
      "Retrieval (TR), Table Metadata Prediction (TMP),\n",
      "and Table Content Population (TCP).\n",
      "\n",
      "3 Training Datasets\n",
      "\n",
      "We present both the datasets used for pre-training\n",
      "and for fine-tuning in the downstream tasks.\n",
      "Pre-training tables are not annotated, in some\n",
      "cases scraped from the web, while data used for\n",
      "fine-tuning have task-dependent annotation labels.\n",
      "The datasets consist of tables and their context,\n",
      "such as table metadata, surrounding texts, claims\n",
      "large pre-training\n",
      "or questions. To construct\n",
      "\n",
      "229\n",
      "\n",
      "l\n",
      "\n",
      "D\n",
      "o\n",
      "w\n",
      "n\n",
      "o\n",
      "a\n",
      "d\n",
      "e\n",
      "d\n",
      "\n",
      "f\n",
      "r\n",
      "o\n",
      "m\n",
      "h\n",
      "\n",
      "t\n",
      "t\n",
      "\n",
      "p\n",
      "\n",
      ":\n",
      "/\n",
      "/\n",
      "\n",
      "d\n",
      "i\n",
      "r\n",
      "e\n",
      "c\n",
      "t\n",
      ".\n",
      "\n",
      "m\n",
      "\n",
      "i\n",
      "t\n",
      ".\n",
      "\n",
      "e\n",
      "d\n",
      "u\n",
      "\n",
      "/\n",
      "t\n",
      "\n",
      "a\n",
      "c\n",
      "l\n",
      "/\n",
      "\n",
      "l\n",
      "\n",
      "a\n",
      "r\n",
      "t\n",
      "i\n",
      "c\n",
      "e\n",
      "-\n",
      "p\n",
      "d\n",
      "\n",
      "f\n",
      "/\n",
      "\n",
      "d\n",
      "o\n",
      "\n",
      "i\n",
      "/\n",
      "\n",
      ".\n",
      "\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "6\n",
      "2\n",
      "\n",
      "/\n",
      "t\n",
      "\n",
      "l\n",
      "\n",
      "a\n",
      "c\n",
      "_\n",
      "a\n",
      "_\n",
      "0\n",
      "0\n",
      "5\n",
      "4\n",
      "4\n",
      "2\n",
      "0\n",
      "7\n",
      "4\n",
      "8\n",
      "7\n",
      "3\n",
      "\n",
      "/\n",
      "\n",
      "/\n",
      "t\n",
      "\n",
      "l\n",
      "\n",
      "a\n",
      "c\n",
      "_\n",
      "a\n",
      "_\n",
      "0\n",
      "0\n",
      "5\n",
      "4\n",
      "4\n",
      "p\n",
      "d\n",
      "\n",
      ".\n",
      "\n",
      "f\n",
      "\n",
      "b\n",
      "y\n",
      "g\n",
      "u\n",
      "e\n",
      "s\n",
      "t\n",
      "\n",
      "o\n",
      "n\n",
      "2\n",
      "3\n",
      "D\n",
      "e\n",
      "c\n",
      "e\n",
      "m\n",
      "b\n",
      "e\n",
      "r\n",
      "2\n",
      "0\n",
      "2\n",
      "3\n",
      "\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "\fdatasets and in an attempt to reduce bias, mul-\n",
      "tiple sources can be used, independently of the\n",
      "target task at hand. For instance, unlike TAPAS\n",
      "(Herzig et al., 2020), which only uses Wikipedia\n",
      "Tables for QA, and TABULARNET (Du et al., 2021),\n",
      "which only uses Spreadsheets for TMP, TABERT\n",
      "(Yin et al., 2020) uses Wikipedia Tables and WDC\n",
      "for SP; GRAPPA uses Wikipedia Tables, Spider,\n",
      "and WikiSQL for SP; MMR (Kosti´c et al., 2021)\n",
      "uses NQ, OTT-QA, and WikiSQL for TR; MATE\n",
      "(Eisenschlos et al., 2021) uses Wikipedia Tables\n",
      "and HybridQA for QA; and TUTA (Wang et al.,\n",
      "2021b) uses Wikipedia Tables, WDC, and spread-\n",
      "sheets for TMP. In general, it is recommended to\n",
      "utilize different data sources for pre-training to\n",
      "ensure covering different kinds and content, and\n",
      "thus, improve the scope of representations. For\n",
      "instance, Wikipedia tables has a large number of\n",
      "relational tables (Bhagavatula et al., 2015), while\n",
      "WDC and Spreadsheets include also entity tables\n",
      "and spreadsheets with complex structure.\n",
      "\n",
      "Table 2 summarizes the main characteristics\n",
      "for the most common datasets. We mark the tasks\n",
      "for which the dataset has been used by ✔ under\n",
      "the column ‘‘Task’’. We note that the top four\n",
      "datasets are mostly used for pre-training, while the\n",
      "others can be used for fine-tuning as well since\n",
      "they include annotations for the target task, for\n",
      "example, questions/answers for QA. The column\n",
      "‘‘Large Tables’’ is a binary indicator, where ✔\n",
      "and ✘ indicate whether or not the tabular corpus\n",
      "include large tables and hence whether or not some\n",
      "pre-processing is needed to reduce table content\n",
      "to meet the limits of the transformer architecture\n",
      "(512 input tokens in most cases). Some works,\n",
      "such as TABERT, TAPEX (Liu et al., 2021a), and\n",
      "CLTR (Pan et al., 2021), apply filtering in any\n",
      "case to reduce noisy input. Finally, the ‘‘Context’’\n",
      "column describes additional text that come with\n",
      "the tables. This can be text describing the table,\n",
      "such as caption or title of the document containing\n",
      "the table; table metadata, such as table orientation,\n",
      "header row, and keys; or questions and claims that\n",
      "can be addressed with the table.\n",
      "\n",
      "4\n",
      "\n",
      "Input Processing\n",
      "\n",
      "As for the original text setting, transformers for\n",
      "tabular data require as input a sequence of tokens.\n",
      "However, in addition to the typical tokenization\n",
      "executed before feeding the text to the neural net-\n",
      "work (Lan et al., 2020), tabular data requires some\n",
      "\n",
      "steps to be processed correctly. Some require-\n",
      "ments come from the nature of the transformer,\n",
      "such as the limitation on the input data size\n",
      "(Section 4.1). Other requirements are due to the na-\n",
      "ture of the tabular data, with structural information\n",
      "expressed in two dimensions that need to be con-\n",
      "verted into a one dimensional space (Section 4.2).\n",
      "Finally, given a table, its data and its context must\n",
      "be jointly fed to the transformer (Section 4.3).\n",
      "\n",
      "4.1 Data Retrieval and Filtering\n",
      "\n",
      "Filtering methods on the table content are applied\n",
      "to stay within the size limits of the transformer\n",
      "architecture, to reduce the model training time, and\n",
      "to eliminate potential noise in the representation.\n",
      "TABERT uses content snapshot to keep the\n",
      "top-k most relevant rows in the table. Such content\n",
      "is identified with the tuples with highest n-gram\n",
      "overlap with respect to the given context (question\n",
      "or utterance). TAPEX and the retrieval model in\n",
      "FEVEROUS (Aly et al., 2021) randomly select rows\n",
      "to limit the input size, while RCI (Glass et al.,\n",
      "2021) down-samples rows using term frequency\n",
      "inverse document frequency (TF-IDF) scores; the\n",
      "frequency can also be used to summarize cells\n",
      "with long text (Li et al., 2020). In addition to\n",
      "keeping tables with number of columns below a\n",
      "fixed threshold, TUTA and TABULARNET split large\n",
      "tables into non-overlapping horizontal partitions.\n",
      "Every partition contains the same header row and\n",
      "it is processed separately by the model. While\n",
      "effective, splitting is more demanding in terms of\n",
      "computation cost.\n",
      "\n",
      "In terms of table selection, whereas for sys-\n",
      "tems like GTR and DTR the objective is to\n",
      "retrieve tables that contain the answer to a given\n",
      "question, others, such as CLTR, use a ranking\n",
      "function, such as BM25 (Robertson et al., 1995),\n",
      "to retrieve relevant tables prior to training, or to\n",
      "generate negative examples, as in MMR. Regard-\n",
      "less of the downstream task, most systems filter\n",
      "and reduce the size of the input data to meet\n",
      "the limits of transformers technology. However,\n",
      "frequency-driven sampling, such as in RCI, is\n",
      "more effective than random as it reduces noise\n",
      "as well in data representations.\n",
      "\n",
      "To summarize, one can group the different\n",
      "content selection strategies based on the targeted\n",
      "downstream task. For TFC, QA, and SP, a se-\n",
      "lection strategy relying on n-gram overlap such\n",
      "as content snapshot or TF-IDF is recommended,\n",
      "\n",
      "230\n",
      "\n",
      "l\n",
      "\n",
      "D\n",
      "o\n",
      "w\n",
      "n\n",
      "o\n",
      "a\n",
      "d\n",
      "e\n",
      "d\n",
      "\n",
      "f\n",
      "r\n",
      "o\n",
      "m\n",
      "h\n",
      "\n",
      "t\n",
      "t\n",
      "\n",
      "p\n",
      "\n",
      ":\n",
      "/\n",
      "/\n",
      "\n",
      "d\n",
      "i\n",
      "r\n",
      "e\n",
      "c\n",
      "t\n",
      ".\n",
      "\n",
      "m\n",
      "\n",
      "i\n",
      "t\n",
      ".\n",
      "\n",
      "e\n",
      "d\n",
      "u\n",
      "\n",
      "/\n",
      "t\n",
      "\n",
      "a\n",
      "c\n",
      "l\n",
      "/\n",
      "\n",
      "l\n",
      "\n",
      "a\n",
      "r\n",
      "t\n",
      "i\n",
      "c\n",
      "e\n",
      "-\n",
      "p\n",
      "d\n",
      "\n",
      "f\n",
      "/\n",
      "\n",
      "d\n",
      "o\n",
      "\n",
      "i\n",
      "/\n",
      "\n",
      ".\n",
      "\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "6\n",
      "2\n",
      "\n",
      "/\n",
      "t\n",
      "\n",
      "l\n",
      "\n",
      "a\n",
      "c\n",
      "_\n",
      "a\n",
      "_\n",
      "0\n",
      "0\n",
      "5\n",
      "4\n",
      "4\n",
      "2\n",
      "0\n",
      "7\n",
      "4\n",
      "8\n",
      "7\n",
      "3\n",
      "\n",
      "/\n",
      "\n",
      "/\n",
      "t\n",
      "\n",
      "l\n",
      "\n",
      "a\n",
      "c\n",
      "_\n",
      "a\n",
      "_\n",
      "0\n",
      "0\n",
      "5\n",
      "4\n",
      "4\n",
      "p\n",
      "d\n",
      "\n",
      ".\n",
      "\n",
      "f\n",
      "\n",
      "b\n",
      "y\n",
      "g\n",
      "u\n",
      "e\n",
      "s\n",
      "t\n",
      "\n",
      "o\n",
      "n\n",
      "2\n",
      "3\n",
      "D\n",
      "e\n",
      "c\n",
      "e\n",
      "m\n",
      "b\n",
      "e\n",
      "r\n",
      "2\n",
      "0\n",
      "2\n",
      "3\n",
      "\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "\fDataset\n",
      "\n",
      "Reference\n",
      "\n",
      "Used for Task\n",
      "TFC QA SP TR TMP TCP\n",
      "\n",
      "Number of\n",
      "Tables\n",
      "\n",
      "Wikipedia Tables\n",
      "\n",
      "Wikipedia\n",
      "\n",
      "✔ ✔ ✔\n",
      "\n",
      "✔\n",
      "\n",
      "✔ 3.2M\n",
      "\n",
      "WDC Web Table Corpus\n",
      "\n",
      "(Lehmberg\n",
      "et al., 2016)\n",
      "\n",
      "✔ ✔\n",
      "\n",
      "✔\n",
      "\n",
      "233M\n",
      "\n",
      "VizNet\n",
      "\n",
      "Spreadsheets\n",
      "\n",
      "NQ-Tables\n",
      "\n",
      "TabFact\n",
      "\n",
      "WikiSQL\n",
      "\n",
      "TabMCQ\n",
      "\n",
      "Spider\n",
      "\n",
      "(Hu et al.,\n",
      "2019)\n",
      "\n",
      "(Dong et al.,\n",
      "2019)\n",
      "\n",
      "(Herzig et al.,\n",
      "2021)\n",
      "\n",
      "(Chen et al.,\n",
      "2020b)\n",
      "\n",
      "(Zhong et al.,\n",
      "2017)\n",
      "\n",
      "(Jauhar et al.,\n",
      "2016)\n",
      "\n",
      "(Yu et al.,\n",
      "2018)\n",
      "\n",
      "WikiTable Question\n",
      "(WikiTQ)\n",
      "\n",
      "(Pasupat and\n",
      "Liang, 2015)\n",
      "\n",
      "✔\n",
      "\n",
      "✔\n",
      "\n",
      "✔ 1M\n",
      "\n",
      "3,410\n",
      "\n",
      "✔\n",
      "\n",
      "✔\n",
      "\n",
      "169,898\n",
      "\n",
      "✔\n",
      "\n",
      "✔ ✔ ✔\n",
      "\n",
      "✔\n",
      "\n",
      "✔\n",
      "\n",
      "✔\n",
      "\n",
      "✔ ✔\n",
      "\n",
      "16K\n",
      "\n",
      "24,241\n",
      "\n",
      "68\n",
      "\n",
      "200 databases\n",
      "\n",
      "2,108\n",
      "\n",
      "Natural Questions (NQ)\n",
      "\n",
      "OTT-QA\n",
      "\n",
      "Web Query Table (WQT)\n",
      "\n",
      "HybridQA\n",
      "\n",
      "FEVEROUS\n",
      "\n",
      "(Kwiatkowski\n",
      "et al., 2019)\n",
      "\n",
      "(Chen et al.,\n",
      "2021)\n",
      "\n",
      "(Sun et al.,\n",
      "2019)\n",
      "\n",
      "(Chen et al.,\n",
      "2020c)\n",
      "\n",
      "(Aly et al.,\n",
      "2021)\n",
      "\n",
      "✔\n",
      "\n",
      "✔\n",
      "\n",
      "✔\n",
      "\n",
      "169, 898∗\n",
      "\n",
      "✔\n",
      "\n",
      "✔\n",
      "\n",
      "400K\n",
      "\n",
      "✔\n",
      "\n",
      "✔\n",
      "\n",
      "273,816\n",
      "\n",
      "13K\n",
      "\n",
      "28.8K\n",
      "\n",
      "Large Tables\n",
      "\n",
      "Context\n",
      "\n",
      "Application Example\n",
      "\n",
      "✔\n",
      "\n",
      "✔\n",
      "\n",
      "✘\n",
      "\n",
      "✘\n",
      "\n",
      "✔\n",
      "\n",
      "✘\n",
      "\n",
      "✘\n",
      "\n",
      "✘\n",
      "\n",
      "✘\n",
      "\n",
      "✘\n",
      "\n",
      "✔\n",
      "\n",
      "✔\n",
      "\n",
      "✘\n",
      "\n",
      "✘\n",
      "\n",
      "✔\n",
      "\n",
      "Surrounding Text: table\n",
      "caption, page title, page\n",
      "description, segment title,\n",
      "text of the segment.\n",
      "Table Metadata:\n",
      "statistics about number of\n",
      "headings, rows, columns,\n",
      "data rows.\n",
      "\n",
      "Table Metadata: Table\n",
      "orientation, header row, key\n",
      "column, timestamp before and\n",
      "after table.\n",
      "Surrounding Text: table caption, text\n",
      "before and after table, title of HTML page.\n",
      "\n",
      "Table Metadata: Column\n",
      "Types.\n",
      "\n",
      "Table Metadata: Cell Roles\n",
      "(Index, Index Name, Value\n",
      "Name, Aggregation and Others).\n",
      "\n",
      "Questions: 12K.\n",
      "\n",
      "Textual Claims: 118K.\n",
      "\n",
      "Questions: 80,654.\n",
      "\n",
      "Questions: 9,092.\n",
      "\n",
      "Questions: 10,181\n",
      "Queries: 5,693.\n",
      "\n",
      "Questions: 22,033.\n",
      "\n",
      "Questions: 320K.\n",
      "\n",
      "Surrounding Text: page title,\n",
      "section title, section text\n",
      "limited to 12 first sentences.\n",
      "Questions: 45,841.\n",
      "\n",
      "Surrounding Text: captions.\n",
      "Queries: 21,113.\n",
      "\n",
      "Questions: 72K.\n",
      "Surrounding Text: first 12\n",
      "sentences per hyperlink in\n",
      "the table.\n",
      "\n",
      "Textual Claims: 87K.\n",
      "Surrounding Text: article title.\n",
      "Table Metadata: row and\n",
      "column headers.\n",
      "\n",
      "TAPAS\n",
      "\n",
      "TABERT\n",
      "\n",
      "TABBIE\n",
      "\n",
      "TABULARNET\n",
      "\n",
      "DTR\n",
      "\n",
      "DECO\n",
      "\n",
      "MMR\n",
      "\n",
      "CLTR\n",
      "\n",
      "GRAPPA\n",
      "\n",
      "TAPEX\n",
      "\n",
      "MMR\n",
      "\n",
      "MMR\n",
      "\n",
      "GTR\n",
      "\n",
      "MATE\n",
      "\n",
      "FEVEROUS\n",
      "\n",
      "Table 2: Datasets for the development and evaluation of neural representation models of tabular data.\n",
      "The top four datasets are mostly used for pre-training models, the rest of the datasets also come with a\n",
      "context and labels that are used in the downstream task for training and evaluation. For Large Tables,\n",
      "✔/✘ denotes whether or not the dataset includes large tables and thus requiring pre-processing to meet\n",
      "transformers’ limits. Application example refers to sample systems using the respective dataset. For\n",
      "the task, we use the acronyms as follows. TFC: Table based Fact-Checking, QA: Question Answering,\n",
      "SP: Semantic Parsing, TR: Table Retrieval, TMP: Table Metadata Prediction, TCP: Table Content\n",
      "Population. ∗: the number of tables is derived from Herzig et al. (2021).\n",
      "\n",
      "while for TR, BM25 is endorsed. In the cases of\n",
      "TMP and TCP, where having the full content of\n",
      "the table is required, such as relation extraction or\n",
      "cell filling, splitting the tabular data without any\n",
      "selection is adopted.\n",
      "\n",
      "4.2 Table Serialization\n",
      "\n",
      "A crucial step is the transformation from the two\n",
      "dimensions of a table to its serialized version con-\n",
      "sumable by the transformer. The methods for table\n",
      "\n",
      "serialization can be grouped into four main types.\n",
      "The first type consists of horizontally scanning\n",
      "the table by row. Most systems achieve this task\n",
      "with a flattened table with value separators, for\n",
      "example, DTR, MMR, TURL (Deng et al., 2020),\n",
      "TAPAS, MATE, and DECO (Yang and Zhu, 2021).\n",
      "For the table in Figure 1, it corresponds to a se-\n",
      "quence such as [CLS] Population in Million by\n",
      "Country | Country | Capital | Population | Australia\n",
      "| Canberra | 25.69 . . . Bolivia | La Paz | 11.67.\n",
      "\n",
      "231\n",
      "\n",
      "l\n",
      "\n",
      "D\n",
      "o\n",
      "w\n",
      "n\n",
      "o\n",
      "a\n",
      "d\n",
      "e\n",
      "d\n",
      "\n",
      "f\n",
      "r\n",
      "o\n",
      "m\n",
      "h\n",
      "\n",
      "t\n",
      "t\n",
      "\n",
      "p\n",
      "\n",
      ":\n",
      "/\n",
      "/\n",
      "\n",
      "d\n",
      "i\n",
      "r\n",
      "e\n",
      "c\n",
      "t\n",
      ".\n",
      "\n",
      "m\n",
      "\n",
      "i\n",
      "t\n",
      ".\n",
      "\n",
      "e\n",
      "d\n",
      "u\n",
      "\n",
      "/\n",
      "t\n",
      "\n",
      "a\n",
      "c\n",
      "l\n",
      "/\n",
      "\n",
      "l\n",
      "\n",
      "a\n",
      "r\n",
      "t\n",
      "i\n",
      "c\n",
      "e\n",
      "-\n",
      "p\n",
      "d\n",
      "\n",
      "f\n",
      "/\n",
      "\n",
      "d\n",
      "o\n",
      "\n",
      "i\n",
      "/\n",
      "\n",
      ".\n",
      "\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "6\n",
      "2\n",
      "\n",
      "/\n",
      "t\n",
      "\n",
      "l\n",
      "\n",
      "a\n",
      "c\n",
      "_\n",
      "a\n",
      "_\n",
      "0\n",
      "0\n",
      "5\n",
      "4\n",
      "4\n",
      "2\n",
      "0\n",
      "7\n",
      "4\n",
      "8\n",
      "7\n",
      "3\n",
      "\n",
      "/\n",
      "\n",
      "/\n",
      "t\n",
      "\n",
      "l\n",
      "\n",
      "a\n",
      "c\n",
      "_\n",
      "a\n",
      "_\n",
      "0\n",
      "0\n",
      "5\n",
      "4\n",
      "4\n",
      "p\n",
      "d\n",
      "\n",
      ".\n",
      "\n",
      "f\n",
      "\n",
      "b\n",
      "y\n",
      "g\n",
      "u\n",
      "e\n",
      "s\n",
      "t\n",
      "\n",
      "o\n",
      "n\n",
      "2\n",
      "3\n",
      "D\n",
      "e\n",
      "c\n",
      "e\n",
      "m\n",
      "b\n",
      "e\n",
      "r\n",
      "2\n",
      "0\n",
      "2\n",
      "3\n",
      "\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "\fet al., 2021; Chen et al., 2020a,e). However,\n",
      "most of these methods show limitations for ta-\n",
      "bles with prevalence of numerical attributes and\n",
      "with missing table context. Within the efforts for\n",
      "table-to-text, graph traversal algorithms have been\n",
      "explored for linearization such as relation-biased\n",
      "breadth first search (Li et al., 2021). While\n",
      "table-to-text generation is an important and chal-\n",
      "lenging task, most methods rely on fine-tuning\n",
      "existing pre-trained language models.\n",
      "\n",
      "While it is still not clear which table serializa-\n",
      "tion method should be used for a given task, few\n",
      "papers performed ablation studies. For instance,\n",
      "TABFACT (Chen et al., 2020b) does not report\n",
      "any significant difference in performance when\n",
      "comparing row and column encoding. TABERT\n",
      "reports that both (i) adding type information for\n",
      "cell values and (ii) phrasing the input as a sentence\n",
      "improve results. The most promising approach is\n",
      "to incorporate several aspects by appending col-\n",
      "umn headers to cell content, combining row and\n",
      "column encoding, or by adding structure-aware\n",
      "indicators as the positional embeddings discussed\n",
      "in Section 5. Finally, there is evidence that textual\n",
      "templates to represent table content is a valid\n",
      "solution when one directly fine-tunes existing\n",
      "pre-trained language models, without performing\n",
      "pre-training on tabular data (Suadaa et al., 2021).\n",
      "\n",
      "4.3 Context and Table Concatenation\n",
      "\n",
      "When available, the table context is concatenated\n",
      "with the table content. Most systems (TABERT,\n",
      "TAPAS, DTR, GRAPPA, and DECO) combine the\n",
      "context by concatenating it in the serialization be-\n",
      "fore the table data, while TAPEX appends it. The\n",
      "authors of UNIFIEDSKG found that placing con-\n",
      "text before the tabular knowledge yields to better\n",
      "results in their setting. In some cases, including\n",
      "CLTR, MMR, and RCI, the table and the context\n",
      "are encoded separately and then are combined at\n",
      "a later stage in the system.\n",
      "\n",
      "Some works do not include context in their\n",
      "pre-training input besides the column headers\n",
      "(TABBIE, TABULARNET, DODUO, GRAPPA). Typi-\n",
      "cally, the decision is based on the target down-\n",
      "stream task. A richer context is used when the\n",
      "tasks are closer to the corresponding NLP task\n",
      "applied on free text. For instance, all models for\n",
      "QA use table captions or descriptions as context.\n",
      "To summarize, including context is crucial for\n",
      "TFC, QA, SP, TR, while it can be excluded for\n",
      "TMP and TCP. In case the context is needed,\n",
      "\n",
      "Figure 2: Examples of context (table caption, in italic)\n",
      "concatenated with row and column data linearization.\n",
      "\n",
      "Another option is a flattened table with special\n",
      "token separators to indicate the beginning of a\n",
      "new row or cell (TAPEX, TUTA, [FORTAP (Cheng\n",
      "et al., 2022)]), as in the first example in Figure 2.\n",
      "Finally, few methods flatten the table representing\n",
      "each cell as a concatenation of column name, col-\n",
      "umn type, and cell value (TABERT), for example,\n",
      "Country | String | Australia [SEP]; or just the row\n",
      "of column headers (GRAPPA).\n",
      "\n",
      "The second linearization type scans the table\n",
      "by column, again either by simple concatenation\n",
      "of column values or by using special tokens as\n",
      "separators, as in DODUO (Suhara et al., 2022). A\n",
      "column serialization for the country population\n",
      "table is the second example in Figure 2.\n",
      "\n",
      "The third linearization type consists of com-\n",
      "bining the output from both types of serialization\n",
      "by using element-wise product (RCI, CLTR), av-\n",
      "erage pooling and concatenation (TABULARNET),\n",
      "or the average of row and column embeddings\n",
      "(TABBIE (Iida et al., 2021)). In a context outside\n",
      "of our framework, which focuses on transformers,\n",
      "it has also been proposed to transform the in-\n",
      "put relation table in a graph and perform random\n",
      "walks on the latter to ultimately produce node\n",
      "embeddings (Cappuzzo et al., 2020).\n",
      "\n",
      "The fourth type consists of using a text tem-\n",
      "plate to represent the tabular data as sentences\n",
      "(Chen et al., 2020b; Suadaa et al., 2021; Chen\n",
      "et al., 2020a). Instead of using predefined tem-\n",
      "plates, natural sentences are generated out of the\n",
      "tabular data, by fine-tuning sequence to sequence\n",
      "language models such as T5 (Raffel et al., 2020) or\n",
      "GPT2 (Radford et al., 2019), as in DRT (Thorne\n",
      "et al., 2021; Neeraja et al., 2021). The gener-\n",
      "ation can rely on models for this table-to-text\n",
      "task, such as TOTTO (Parikh et al., 2020), PYTHIA\n",
      "(Veltri et al., 2022; 2023), TABLEGPT (Gong et al.,\n",
      "2020), LOGIC2TEXT (Chen et al., 2020d), UNIFIED-\n",
      "SKG (Xie et al., 2022), or other efforts (Suadaa\n",
      "\n",
      "232\n",
      "\n",
      "l\n",
      "\n",
      "D\n",
      "o\n",
      "w\n",
      "n\n",
      "o\n",
      "a\n",
      "d\n",
      "e\n",
      "d\n",
      "\n",
      "f\n",
      "r\n",
      "o\n",
      "m\n",
      "h\n",
      "\n",
      "t\n",
      "t\n",
      "\n",
      "p\n",
      "\n",
      ":\n",
      "/\n",
      "/\n",
      "\n",
      "d\n",
      "i\n",
      "r\n",
      "e\n",
      "c\n",
      "t\n",
      ".\n",
      "\n",
      "m\n",
      "\n",
      "i\n",
      "t\n",
      ".\n",
      "\n",
      "e\n",
      "d\n",
      "u\n",
      "\n",
      "/\n",
      "t\n",
      "\n",
      "a\n",
      "c\n",
      "l\n",
      "/\n",
      "\n",
      "l\n",
      "\n",
      "a\n",
      "r\n",
      "t\n",
      "i\n",
      "c\n",
      "e\n",
      "-\n",
      "p\n",
      "d\n",
      "\n",
      "f\n",
      "/\n",
      "\n",
      "d\n",
      "o\n",
      "\n",
      "i\n",
      "/\n",
      "\n",
      ".\n",
      "\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "6\n",
      "2\n",
      "\n",
      "/\n",
      "t\n",
      "\n",
      "l\n",
      "\n",
      "a\n",
      "c\n",
      "_\n",
      "a\n",
      "_\n",
      "0\n",
      "0\n",
      "5\n",
      "4\n",
      "4\n",
      "2\n",
      "0\n",
      "7\n",
      "4\n",
      "8\n",
      "7\n",
      "3\n",
      "\n",
      "/\n",
      "\n",
      "/\n",
      "t\n",
      "\n",
      "l\n",
      "\n",
      "a\n",
      "c\n",
      "_\n",
      "a\n",
      "_\n",
      "0\n",
      "0\n",
      "5\n",
      "4\n",
      "4\n",
      "p\n",
      "d\n",
      "\n",
      ".\n",
      "\n",
      "f\n",
      "\n",
      "b\n",
      "y\n",
      "g\n",
      "u\n",
      "e\n",
      "s\n",
      "t\n",
      "\n",
      "o\n",
      "n\n",
      "2\n",
      "3\n",
      "D\n",
      "e\n",
      "c\n",
      "e\n",
      "m\n",
      "b\n",
      "e\n",
      "r\n",
      "2\n",
      "0\n",
      "2\n",
      "3\n",
      "\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "\fprepending or appending it to the table content\n",
      "does not modify the performance of the model.\n",
      "\n",
      "architecture is used for models that focus on\n",
      "sequence generation tasks (RPT, TAPEX).\n",
      "\n",
      "5 Adaptation of Transformers\n",
      "\n",
      "thus\n",
      "\n",
      "To account for structured tables in the input,\n",
      "several pre-trained transformer-based LM and\n",
      "systems have been developed. Vanilla LMs are\n",
      "customized to make the model more ‘‘data\n",
      "structure-aware’’,\n",
      "rendering a modified\n",
      "transformer-based encoder to be utilized on other\n",
      "tasks. These encoders, depicted in part (1) of\n",
      "Figure 1, capture both structure and semantic in-\n",
      "formation. To exploit such resources and build\n",
      "applications, as in part (2) of Figure 1, several\n",
      "systems build on top of the encoder, usually with\n",
      "more modules and fine-tuning. In a different ap-\n",
      "proach, other systems use the encoder as part of a\n",
      "bigger architecture in a more task-oriented fash-\n",
      "ion rather than encoder-oriented. We first briefly\n",
      "revise the vanilla transformer architecture, then\n",
      "discuss customizations to LMs.\n",
      "\n",
      "5.1 Vanilla Transformer\n",
      "\n",
      "The vanilla transformer (Vaswani et al., 2017) is\n",
      "a seq2seq model (Sutskever et al., 2014) consist-\n",
      "ing of an encoder and a decoder, each of which\n",
      "is a stack of N identical modules. The encoder\n",
      "block is composed of a multi-head self-attention\n",
      "module and a position-wise feed-forward network.\n",
      "Multi-head attention allows the model to jointly\n",
      "attend to information from different representation\n",
      "subspaces at different positions. Residual connec-\n",
      "tions and layer-normalization modules are also\n",
      "used. Decoder blocks consist of cross-attention\n",
      "modules between the multi-head self-attention\n",
      "modules and the position-wise feed-forward net-\n",
      "works, where masking is used to prevent each\n",
      "position from attending to subsequent positions.\n",
      "\n",
      "The transformer architecture can be used as\n",
      "an encoder-decoder (Vaswani et al., 2017; Raffel\n",
      "et al., 2020), an encoder-only (Devlin et al., 2019;\n",
      "Liu et al., 2019), or decoder-only (Radford et al.,\n",
      "2019; Brown et al., 2020) model. The choice\n",
      "of the architecture depends on the final task.\n",
      "Encoder-only models are mainly used for clas-\n",
      "sification and are the most popular choice for\n",
      "extensions for tabular data. In this case, pre-\n",
      "training is done with a masked language model-\n",
      "ing (MLM) task, whose goal is to predict masked\n",
      "token(s) of an altered input. The encoder-decoder\n",
      "\n",
      "5.2 LM Extensions for Tabular Data\n",
      "\n",
      "To properly model the structure of data in tables,\n",
      "the vanilla transformers are extended and updated\n",
      "by modifying components at the (i) input, (ii)\n",
      "internal, (iii) output, and (iv) training-procedure\n",
      "levels. We discuss each of them in the following.\n",
      "A summary of the extensions is provided as a\n",
      "taxonomy in Figure 3. We note that since the en-\n",
      "coder and decoder modules of a transformer share\n",
      "similar structure, most of these modifications can\n",
      "be applied to both.\n",
      "\n",
      "5.2.1 Input Level\n",
      "\n",
      "Modifications on the input level are usually desig-\n",
      "nated with additional positional embeddings to\n",
      "explicitly model the table structure. TABERT and\n",
      "TAPAS show how such embeddings improve per-\n",
      "formance on structure-related tasks. For example,\n",
      "embeddings that represent the position of the cell,\n",
      "indicated by its row and column IDs, are common\n",
      "for relational tables, for example, in TABERT,\n",
      "TAPAS, and TABBIE. TABLEFORMER (Yang et al.,\n",
      "2022) drops row and column ids to avoid any po-\n",
      "tential row and column biases and they instead use\n",
      "per cell positional embeddings similar to MATE.\n",
      "For tables without a relational structure, such as\n",
      "entity tables and spreadsheets, including complex\n",
      "financial tables, TUTA introduces tree-based posi-\n",
      "tional embeddings to encode the position of a cell\n",
      "using top and left embeddings of a bi-dimensional\n",
      "coordinate tree. Other supplementary embeddings\n",
      "include those that provide relative positional in-\n",
      "formation for a token within a caption/header\n",
      "(TURL) or a cell (TUTA). For tasks such as QA,\n",
      "segment embeddings are used to differentiate be-\n",
      "tween the different input types, question and table,\n",
      "for example, in RPT (Tang et al., 2021) and\n",
      "TAPAS. Finally, TUTA introduces embeddings for\n",
      "numbers when discrete features are used.\n",
      "\n",
      "While row/column positional embeddings can\n",
      "better map context and table content, such as in QA\n",
      "or TFC tasks, it cannot overcome the challenge\n",
      "of empty cells, nested row headers, or descriptive\n",
      "cells in complex spreadsheets such as financial\n",
      "tables. In this case, tree-based positional embed-\n",
      "dings are required. Aside from new positional\n",
      "embeddings encoding the structure of a token in\n",
      "a table, original, vanilla positional embeddings\n",
      "can also be modified for a better representation of\n",
      "\n",
      "233\n",
      "\n",
      "l\n",
      "\n",
      "D\n",
      "o\n",
      "w\n",
      "n\n",
      "o\n",
      "a\n",
      "d\n",
      "e\n",
      "d\n",
      "\n",
      "f\n",
      "r\n",
      "o\n",
      "m\n",
      "h\n",
      "\n",
      "t\n",
      "t\n",
      "\n",
      "p\n",
      "\n",
      ":\n",
      "/\n",
      "/\n",
      "\n",
      "d\n",
      "i\n",
      "r\n",
      "e\n",
      "c\n",
      "t\n",
      ".\n",
      "\n",
      "m\n",
      "\n",
      "i\n",
      "t\n",
      ".\n",
      "\n",
      "e\n",
      "d\n",
      "u\n",
      "\n",
      "/\n",
      "t\n",
      "\n",
      "a\n",
      "c\n",
      "l\n",
      "/\n",
      "\n",
      "l\n",
      "\n",
      "a\n",
      "r\n",
      "t\n",
      "i\n",
      "c\n",
      "e\n",
      "-\n",
      "p\n",
      "d\n",
      "\n",
      "f\n",
      "/\n",
      "\n",
      "d\n",
      "o\n",
      "\n",
      "i\n",
      "/\n",
      "\n",
      ".\n",
      "\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "6\n",
      "2\n",
      "\n",
      "/\n",
      "t\n",
      "\n",
      "l\n",
      "\n",
      "a\n",
      "c\n",
      "_\n",
      "a\n",
      "_\n",
      "0\n",
      "0\n",
      "5\n",
      "4\n",
      "4\n",
      "2\n",
      "0\n",
      "7\n",
      "4\n",
      "8\n",
      "7\n",
      "3\n",
      "\n",
      "/\n",
      "\n",
      "/\n",
      "t\n",
      "\n",
      "l\n",
      "\n",
      "a\n",
      "c\n",
      "_\n",
      "a\n",
      "_\n",
      "0\n",
      "0\n",
      "5\n",
      "4\n",
      "4\n",
      "p\n",
      "d\n",
      "\n",
      ".\n",
      "\n",
      "f\n",
      "\n",
      "b\n",
      "y\n",
      "g\n",
      "u\n",
      "e\n",
      "s\n",
      "t\n",
      "\n",
      "o\n",
      "n\n",
      "2\n",
      "3\n",
      "D\n",
      "e\n",
      "c\n",
      "e\n",
      "m\n",
      "b\n",
      "e\n",
      "r\n",
      "2\n",
      "0\n",
      "2\n",
      "3\n",
      "\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "\fl\n",
      "\n",
      "D\n",
      "o\n",
      "w\n",
      "n\n",
      "o\n",
      "a\n",
      "d\n",
      "e\n",
      "d\n",
      "\n",
      "f\n",
      "r\n",
      "o\n",
      "m\n",
      "h\n",
      "\n",
      "t\n",
      "t\n",
      "\n",
      "p\n",
      "\n",
      ":\n",
      "/\n",
      "/\n",
      "\n",
      "d\n",
      "i\n",
      "r\n",
      "e\n",
      "c\n",
      "t\n",
      ".\n",
      "\n",
      "m\n",
      "\n",
      "i\n",
      "t\n",
      ".\n",
      "\n",
      "e\n",
      "d\n",
      "u\n",
      "\n",
      "/\n",
      "t\n",
      "\n",
      "a\n",
      "c\n",
      "l\n",
      "/\n",
      "\n",
      "l\n",
      "\n",
      "a\n",
      "r\n",
      "t\n",
      "i\n",
      "c\n",
      "e\n",
      "-\n",
      "p\n",
      "d\n",
      "\n",
      "f\n",
      "/\n",
      "\n",
      "d\n",
      "o\n",
      "\n",
      "i\n",
      "/\n",
      "\n",
      ".\n",
      "\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "6\n",
      "2\n",
      "\n",
      "/\n",
      "t\n",
      "\n",
      "l\n",
      "\n",
      "a\n",
      "c\n",
      "_\n",
      "a\n",
      "_\n",
      "0\n",
      "0\n",
      "5\n",
      "4\n",
      "4\n",
      "2\n",
      "0\n",
      "7\n",
      "4\n",
      "8\n",
      "7\n",
      "3\n",
      "\n",
      "/\n",
      "\n",
      "/\n",
      "t\n",
      "\n",
      "l\n",
      "\n",
      "a\n",
      "c\n",
      "_\n",
      "a\n",
      "_\n",
      "0\n",
      "0\n",
      "5\n",
      "4\n",
      "4\n",
      "p\n",
      "d\n",
      "\n",
      ".\n",
      "\n",
      "f\n",
      "\n",
      "b\n",
      "y\n",
      "g\n",
      "u\n",
      "e\n",
      "s\n",
      "t\n",
      "\n",
      "o\n",
      "n\n",
      "2\n",
      "3\n",
      "D\n",
      "e\n",
      "c\n",
      "e\n",
      "m\n",
      "b\n",
      "e\n",
      "r\n",
      "2\n",
      "0\n",
      "2\n",
      "3\n",
      "\n",
      "Figure 3: Taxonomy of extensions to transformer-based LMs for handling tabular data. We report as leaves a\n",
      "representative system for every extension.\n",
      "\n",
      "tokens within table cells. For example, TABERT\n",
      "employs a pre-training task that explicitly uses\n",
      "positional embeddings, alongside a cell represen-\n",
      "tation, for the goal of recovering a cell value. In\n",
      "this case, the original positional embeddings help\n",
      "better model multiple tokens in a cell.\n",
      "\n",
      "5.2.2 Internal Level\n",
      "\n",
      "the modifications on the internal\n",
      "Most of\n",
      "level are applied to make the system more\n",
      "the attention\n",
      "‘‘structure-aware’’. Specifically,\n",
      "module is updated to integrate the structure of\n",
      "the input table. For example, in TABERT verti-\n",
      "cal self-attention layers are produced to capture\n",
      "cross-row dependencies on cell values by per-\n",
      "forming the attention module in a vertical fashion.\n",
      "Empirical results in TUTA show that employ-\n",
      "ing row-wise and column-wise attention, instead\n",
      "\n",
      "of having additional positional embeddings for\n",
      "rows and columns, hurts model performance for\n",
      "cell-type classification tasks, but it is not the case\n",
      "for table-type classification tasks.\n",
      "\n",
      "Other systems, such as TURL, employ a masked\n",
      "self-attention module, which attends to struc-\n",
      "turally related elements such as those in the same\n",
      "row or column, thus ignoring the other elements,\n",
      "unlike the traditional transformer where each ele-\n",
      "ment attends to all other elements in the sequence.\n",
      "Moreover, for categorical data, named entities,\n",
      "such as city names, can be identified in the cell\n",
      "values. In TURL and TUTA, masking such entities\n",
      "helps the models capture the factual knowledge\n",
      "embedded in the table content as well as the asso-\n",
      "ciations between table metadata and table content.\n",
      "Ablation studies showed the positive impact of\n",
      "these two modifications on model performance.\n",
      "\n",
      "234\n",
      "\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "\fOther modifications address the input size con-\n",
      "straint of attention modules, where large tables\n",
      "are often neglected. Sparse attention methods are\n",
      "proposed to cope with this issue (Tay et al., 2020).\n",
      "For instance, MATE sparsifies the attention matrix\n",
      "to allow transformer heads to efficiently attend to\n",
      "either rows or columns.\n",
      "\n",
      "Modifications to the input level, by additional\n",
      "input embeddings to encode cell positions (TAPAS),\n",
      "help in tasks requiring information at the cell level,\n",
      "such as QA and cell type classification. However,\n",
      "tasks requiring understanding of table structure,\n",
      "such as table type classification, benefit more from\n",
      "modifying the attention module (TABERT). Sys-\n",
      "tems modifying both, such as TUTA, seem to be the\n",
      "best option. However, modifying the internal level\n",
      "is more effective, as removing such modification\n",
      "leads to the larger decrease in performance.\n",
      "\n",
      "5.2.3 Output Level\n",
      "\n",
      "Additional layers can be added on top of the\n",
      "feed-forward networks (FFNs) of the LM de-\n",
      "pending on the task at hand. Tasks such as cell\n",
      "selection (TAPAS), TFC (TABFACT), TMP (DODUO),\n",
      "and QA (TAPAS) require to train one or more addi-\n",
      "tional layers. Classification layers for aggregation\n",
      "operations and cell-selection are used to support\n",
      "aggregating cell values (GRAPPA, MATE, DODUO,\n",
      "DECO, RCI). In TAPEX, aggregation operations are\n",
      "also ‘‘learned’’ end-to-end in a seq2seq task.\n",
      "\n",
      "5.2.4 Training-Procedure Level\n",
      "\n",
      "Modifications on the training-procedure level can\n",
      "be attributed to the pre-training task and objective.\n",
      "Pre-training Tasks. Systems are either trained\n",
      "end-to-end or fine-tuned after some pre-training\n",
      "procedure. Almost all pre-training tasks fall un-\n",
      "der the category of reconstruction tasks, where\n",
      "the idea is to reconstruct the correct input from\n",
      "a corrupted one. Rather than applying traditional\n",
      "token-level MLM on the serialized tabular data, ef-\n",
      "fectively treating it as natural language sequences,\n",
      "most pre-training tasks are designed to consider\n",
      "the information about the table structure.\n",
      "\n",
      "Typically, traditional masking of the tokens is\n",
      "used for both, the textual context surrounding the\n",
      "table and the table cells (e.g., TAPAS 15%, TURL\n",
      "20%) with some novelties for table cells such as\n",
      "masked columns (TABERT) and masked entities\n",
      "(TURL). Specifically, TAPAS follows the strategy\n",
      "to mask the whole cell table whenever a token in\n",
      "that cell is randomly masked. Inspired by TAPAS,\n",
      "\n",
      "TUTA masks 15% of the cells by randomly select-\n",
      "ing those that consist mainly of text, rather than\n",
      "numerical values. Among these selected cells,\n",
      "30% are masked completely, while for 70% only\n",
      "a single token within the cell is masked. Mask-\n",
      "ing column names and data types for relational\n",
      "tables encourages the model to recover the meta-\n",
      "data, while masking tokens and whole cell values\n",
      "ensure that data information is retained across\n",
      "the different vertical self-attention layers. Indeed,\n",
      "masking at the entity level enables the model to\n",
      "integrate the factual knowledge embedded in the\n",
      "table content and its context.\n",
      "\n",
      "In TABBIE and RPT, the pre-training tasks\n",
      "detect if a cell/tuple is corrupted or not. While\n",
      "corruption has been shown to outperform MLM\n",
      "for standard textual LMs (Clark et al., 2020), it\n",
      "is not clear if the benefit generalizes to tabular\n",
      "setting. In TUTA, text headers of tables are used to\n",
      "learn representations of tables in a self-supervised\n",
      "manner, i.e., a binary classification task where\n",
      "a positive example is the table and its associ-\n",
      "ated header, and a negative example is any other\n",
      "header.\n",
      "\n",
      "While most systems pre-train for the purpose of\n",
      "learning initial tabular representations, performing\n",
      "aggregations and numerical operations is usually\n",
      "adapted by fine-tuning a classifier for predicting\n",
      "cells and operations, e.g., in TAPAS. However,\n",
      "systems such as GRAPPA and TAPEX pre-train\n",
      "with SQL queries. In GRAPPA, the pre-training task\n",
      "enables the discovery of suitable fragments for SP\n",
      "by pre-training on a large number of queries, with\n",
      "SQL logic grounding to each table, which might\n",
      "help generalize to other similar queries. Empirical\n",
      "results show a decrease in performance without\n",
      "this pre-training task. TAPEX enforces the LM to\n",
      "emulate a SQL engine by pre-training on sampled\n",
      "tables and synthesized queries. This pre-training\n",
      "task gives (i) the LM a deeper understanding of\n",
      "the tables, as the system encounters operations\n",
      "such as selection and aggregation rendering its\n",
      "representations ‘aware’ of such operations, and\n",
      "(ii) improves the performance in QA.\n",
      "\n",
      "According to (Chang et al., 2020), a pre-training\n",
      "task should be cost-efficient, ideally not requir-\n",
      "ing additional human supervision. As manually\n",
      "annotating queries with tables is costly, using sur-\n",
      "rounding information, such as the table header or\n",
      "caption, as a query acts as a self-supervised signal,\n",
      "helping the system learn better representations for\n",
      "the tables. It is the case for TR systems, such\n",
      "\n",
      "235\n",
      "\n",
      "l\n",
      "\n",
      "D\n",
      "o\n",
      "w\n",
      "n\n",
      "o\n",
      "a\n",
      "d\n",
      "e\n",
      "d\n",
      "\n",
      "f\n",
      "r\n",
      "o\n",
      "m\n",
      "h\n",
      "\n",
      "t\n",
      "t\n",
      "\n",
      "p\n",
      "\n",
      ":\n",
      "/\n",
      "/\n",
      "\n",
      "d\n",
      "i\n",
      "r\n",
      "e\n",
      "c\n",
      "t\n",
      ".\n",
      "\n",
      "m\n",
      "\n",
      "i\n",
      "t\n",
      ".\n",
      "\n",
      "e\n",
      "d\n",
      "u\n",
      "\n",
      "/\n",
      "t\n",
      "\n",
      "a\n",
      "c\n",
      "l\n",
      "/\n",
      "\n",
      "l\n",
      "\n",
      "a\n",
      "r\n",
      "t\n",
      "i\n",
      "c\n",
      "e\n",
      "-\n",
      "p\n",
      "d\n",
      "\n",
      "f\n",
      "/\n",
      "\n",
      "d\n",
      "o\n",
      "\n",
      "i\n",
      "/\n",
      "\n",
      ".\n",
      "\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "6\n",
      "2\n",
      "\n",
      "/\n",
      "t\n",
      "\n",
      "l\n",
      "\n",
      "a\n",
      "c\n",
      "_\n",
      "a\n",
      "_\n",
      "0\n",
      "0\n",
      "5\n",
      "4\n",
      "4\n",
      "2\n",
      "0\n",
      "7\n",
      "4\n",
      "8\n",
      "7\n",
      "3\n",
      "\n",
      "/\n",
      "\n",
      "/\n",
      "t\n",
      "\n",
      "l\n",
      "\n",
      "a\n",
      "c\n",
      "_\n",
      "a\n",
      "_\n",
      "0\n",
      "0\n",
      "5\n",
      "4\n",
      "4\n",
      "p\n",
      "d\n",
      "\n",
      ".\n",
      "\n",
      "f\n",
      "\n",
      "b\n",
      "y\n",
      "g\n",
      "u\n",
      "e\n",
      "s\n",
      "t\n",
      "\n",
      "o\n",
      "n\n",
      "2\n",
      "3\n",
      "D\n",
      "e\n",
      "c\n",
      "e\n",
      "m\n",
      "b\n",
      "e\n",
      "r\n",
      "2\n",
      "0\n",
      "2\n",
      "3\n",
      "\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "\fModule Level\n",
      "\n",
      "(Pre-)Training\n",
      "\n",
      "System\n",
      "\n",
      "Input:\n",
      "\n",
      "Internal:\n",
      "\n",
      "Output: FFN\n",
      "\n",
      "Task\n",
      "\n",
      "Objective\n",
      "\n",
      "Addition. Pos. Emb.\n",
      "\n",
      "Attention\n",
      "\n",
      "Encod. Level\n",
      "\n",
      "Task-Orien Mask\n",
      "\n",
      "Corrupt Oth Cls Rnk\n",
      "\n",
      "Ro Co Tr Nu Fr Vrt Sp Tr Vi Ro Co Tb Ce To CP Nli Sp Ag To Ce Co TC Tu Ce Nse CE PR\n",
      "\n",
      "X X X\n",
      "\n",
      "X\n",
      "\n",
      "X\n",
      "\n",
      "X\n",
      "\n",
      "X\n",
      "\n",
      "TUTA\n",
      "TURL\n",
      "TABERT\n",
      "TABBIE X X\n",
      "X X\n",
      "MATE\n",
      "RCI\n",
      "GRAPPA\n",
      "TAPEX\n",
      "DODUO\n",
      "TABFACT\n",
      "RPT\n",
      "TAPAS\n",
      "GTR\n",
      "\n",
      "X X\n",
      "\n",
      "X X\n",
      "\n",
      "X X\n",
      "X\n",
      "\n",
      "X\n",
      "\n",
      "X\n",
      "\n",
      "X X\n",
      "\n",
      "X X\n",
      "X X X\n",
      "X\n",
      "X X\n",
      "\n",
      "X X\n",
      "\n",
      "X\n",
      "X X\n",
      "X X\n",
      "\n",
      "X\n",
      "\n",
      "X\n",
      "\n",
      "X\n",
      "X X\n",
      "\n",
      "X X\n",
      "\n",
      "X\n",
      "\n",
      "X X\n",
      "\n",
      "X\n",
      "\n",
      "X\n",
      "\n",
      "X\n",
      "\n",
      "X\n",
      "\n",
      "X\n",
      "\n",
      "X\n",
      "X X\n",
      "X X X\n",
      "\n",
      "X\n",
      "\n",
      "X\n",
      "\n",
      "X\n",
      "X\n",
      "X\n",
      "\n",
      "X\n",
      "\n",
      "X\n",
      "X\n",
      "X\n",
      "X\n",
      "X\n",
      "\n",
      "X\n",
      "X\n",
      "X\n",
      "X\n",
      "\n",
      "X\n",
      "\n",
      "Table 3: Transformer customizations for every system. The acronyms from left to right follow the same\n",
      "order of the leaves of the taxonomy tree in Figure 3 from top to bottom.\n",
      "\n",
      "as DTR and GTR, where performance improves\n",
      "when using a pre-training task rather than when\n",
      "training from scratch.\n",
      "\n",
      "To summarize, token-level masking is the base\n",
      "for all the systems and it can be sufficient for\n",
      "TFC and TR tasks, while cell or entity masking\n",
      "are also recommended for QA, TMP, such as cell\n",
      "role classification, and TCP, such as cell filling.\n",
      "Column masking is additionally suggested for\n",
      "SP since it helps in identifying the columns to\n",
      "formulate the logical queries.\n",
      "\n",
      "Pre-training Objectives. The objective of the\n",
      "majority of the systems is to minimize cross-\n",
      "entropy loss for a certain classification task. GTR\n",
      "utilizes a point-wise ranking objective for end-\n",
      "to-end training after pre-training, where multiple\n",
      "tables are ranked according to a relevance score\n",
      "given a certain query.\n",
      "\n",
      "Table 3 reports a summary of the transformer\n",
      "customizations adopted by every system. Overall,\n",
      "a tabular LM requires modifications to the input\n",
      "level, through additional embeddings, and to the\n",
      "internal level, through adjustments of the atten-\n",
      "tion module. Pre-training on table-related tasks,\n",
      "such as masking or corrupting table cells, also\n",
      "enhances encoding capabilities for structured data\n",
      "on fine-tuned tasks. Modifications on the output\n",
      "level are more task specific and have less im-\n",
      "pact on making the LM ‘‘understand’’ the data\n",
      "structure.\n",
      "\n",
      "6 Downstream Tasks\n",
      "\n",
      "Using neural representations for tabular data show,\n",
      "improvements in performance in several down-\n",
      "stream tasks. In this section, we describe the\n",
      "tasks and define their input and output. While\n",
      "they all consume tables, settings can be quite\n",
      "heterogeneous, with systems exploiting different\n",
      "information, even for the same task. A summary\n",
      "of the covered tasks along their input, output, and\n",
      "some representative systems addressing them is\n",
      "shown in Table 4. We detail next the mandatory\n",
      "input elements and the different contexts.\n",
      "\n",
      "Table-based Fact-Checking (TFC): Similar to\n",
      "text-based textual entailment (Dagan et al., 2013;\n",
      "Korman et al., 2018), checking facts with tables\n",
      "consists of verifying if a textual input claim is\n",
      "true or false against a trusted database (TAPEX,\n",
      "DECO, TABFACT), also provided as input. Some\n",
      "fact-checking systems, such as FEVEROUS, also\n",
      "output the cells used for verification as evidence\n",
      "(Nakov et al., 2021; Karagiannis et al., 2020).\n",
      "\n",
      "Question Answering (QA):\n",
      "In the free text set-\n",
      "ting, QA aims at retrieving passages that include\n",
      "the answer to a given question. In the tabular data\n",
      "setting, it consists of returning as output the cells\n",
      "that answer an input consisting of a question and\n",
      "a table. One can distinguish two levels of com-\n",
      "plexity. Simple QA involves lookup queries on\n",
      "tables (DTR, CLTR), while a more complex QA\n",
      "\n",
      "236\n",
      "\n",
      "l\n",
      "\n",
      "D\n",
      "o\n",
      "w\n",
      "n\n",
      "o\n",
      "a\n",
      "d\n",
      "e\n",
      "d\n",
      "\n",
      "f\n",
      "r\n",
      "o\n",
      "m\n",
      "h\n",
      "\n",
      "t\n",
      "t\n",
      "\n",
      "p\n",
      "\n",
      ":\n",
      "/\n",
      "/\n",
      "\n",
      "d\n",
      "i\n",
      "r\n",
      "e\n",
      "c\n",
      "t\n",
      ".\n",
      "\n",
      "m\n",
      "\n",
      "i\n",
      "t\n",
      ".\n",
      "\n",
      "e\n",
      "d\n",
      "u\n",
      "\n",
      "/\n",
      "t\n",
      "\n",
      "a\n",
      "c\n",
      "l\n",
      "/\n",
      "\n",
      "l\n",
      "\n",
      "a\n",
      "r\n",
      "t\n",
      "i\n",
      "c\n",
      "e\n",
      "-\n",
      "p\n",
      "d\n",
      "\n",
      "f\n",
      "/\n",
      "\n",
      "d\n",
      "o\n",
      "\n",
      "i\n",
      "/\n",
      "\n",
      ".\n",
      "\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "6\n",
      "2\n",
      "\n",
      "/\n",
      "t\n",
      "\n",
      "l\n",
      "\n",
      "a\n",
      "c\n",
      "_\n",
      "a\n",
      "_\n",
      "0\n",
      "0\n",
      "5\n",
      "4\n",
      "4\n",
      "2\n",
      "0\n",
      "7\n",
      "4\n",
      "8\n",
      "7\n",
      "3\n",
      "\n",
      "/\n",
      "\n",
      "/\n",
      "t\n",
      "\n",
      "l\n",
      "\n",
      "a\n",
      "c\n",
      "_\n",
      "a\n",
      "_\n",
      "0\n",
      "0\n",
      "5\n",
      "4\n",
      "4\n",
      "p\n",
      "d\n",
      "\n",
      ".\n",
      "\n",
      "f\n",
      "\n",
      "b\n",
      "y\n",
      "g\n",
      "u\n",
      "e\n",
      "s\n",
      "t\n",
      "\n",
      "o\n",
      "n\n",
      "2\n",
      "3\n",
      "D\n",
      "e\n",
      "c\n",
      "e\n",
      "m\n",
      "b\n",
      "e\n",
      "r\n",
      "2\n",
      "0\n",
      "2\n",
      "3\n",
      "\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "\fTask ID\n",
      "\n",
      "Task Label\n",
      "\n",
      "Tasks Coverage\n",
      "\n",
      "Input\n",
      "\n",
      "Output\n",
      "\n",
      "System Examples\n",
      "\n",
      "TFC\n",
      "\n",
      "QA\n",
      "\n",
      "Table-based\n",
      "Fact-Checking\n",
      "\n",
      "Fact-Checking Text\n",
      "Refusal/Entailment\n",
      "\n",
      "Table + Claim\n",
      "\n",
      "True/False\n",
      "Refused/Entailed\n",
      "(Data Evidence)\n",
      "\n",
      "Question\n",
      "Answering\n",
      "\n",
      "Retrieving the Cells\n",
      "for the Answer\n",
      "\n",
      "Table + Question\n",
      "\n",
      "Answer Cells\n",
      "\n",
      "SP\n",
      "\n",
      "Semantic Parsing\n",
      "\n",
      "Text-to-SQL\n",
      "\n",
      "Table + NL Query\n",
      "\n",
      "Formal QL\n",
      "\n",
      "TR\n",
      "\n",
      "Table Retrieval\n",
      "\n",
      "TMP\n",
      "\n",
      "Table Metadata\n",
      "Prediction\n",
      "\n",
      "Retrieving Table that\n",
      "Contains the Answer\n",
      "\n",
      "Column Type Prediction\n",
      "Table Type Classification\n",
      "Header Detection Cell\n",
      "Role Classification Column\n",
      "Relation Annotation Column\n",
      "Name Prediction\n",
      "\n",
      "Tables + Question\n",
      "\n",
      "Relevant Table(s)\n",
      "\n",
      "Table\n",
      "\n",
      "Column Types\n",
      "Table Types\n",
      "Header Row\n",
      "Cell Role\n",
      "Relation between Two Cols\n",
      "Column Name\n",
      "\n",
      "TCP\n",
      "\n",
      "Table Content\n",
      "Population\n",
      "\n",
      "Cell Content Population\n",
      "\n",
      "Table with Corrupted\n",
      "Cell Values\n",
      "\n",
      "Table with Complete\n",
      "Cell Values\n",
      "\n",
      "Table 4: List of tasks utilizing neural representations for tables.\n",
      "\n",
      "DECO\n",
      "TABFACT\n",
      "TAPEX\n",
      "\n",
      "TAPAS\n",
      "MATE\n",
      "DTR\n",
      "\n",
      "TABERT\n",
      "GRAPPA\n",
      "TAPEX\n",
      "\n",
      "GTR\n",
      "CLTR\n",
      "\n",
      "DODUO\n",
      "TABULARNET\n",
      "TURL\n",
      "TUTA\n",
      "\n",
      "TURL\n",
      "TABBIE\n",
      "RPT\n",
      "\n",
      "task involves aggregation operations and numeri-\n",
      "cal reasoning (TAPAS, MATE, RCI, DRT, TAPEX).\n",
      "Most of the systems in this survey aim at improv-\n",
      "ing accuracy in QA with respect to hand-crafted\n",
      "embeddings.\n",
      "\n",
      "Semantic Parsing (SP):\n",
      "In the tabular data set-\n",
      "ting, given a question and a table as input, SP\n",
      "generates a declarative query in SQL over the\n",
      "table’s schema to retrieve the answer to the ques-\n",
      "tion. While in QA the interest is in directly getting\n",
      "the answer, SP produces the (interpretable) query\n",
      "to obtain it (TABERT, GRAPPA, TAPEX).\n",
      "\n",
      "Table Retrieval (TR): Given a question and a\n",
      "set of tables as inputs, TR identifies the table that\n",
      "can be used to answer the question. TR is helpful\n",
      "when trying to reduce the search space for a QA\n",
      "task (GTR, DTR, MMR). It is a challenging task\n",
      "given the limited input size of transformers, that\n",
      "is, their constraint to sequences of 512 tokens.\n",
      "\n",
      "Table Metadata Prediction (TMP): Given an\n",
      "input table with corrupted or missing metadata,\n",
      "the TMP objective is to predict inter-table meta-\n",
      "data, such as column types and headers, cell\n",
      "types, table types, and intra-tables relationships,\n",
      "such as equivalence between columns and entity\n",
      "linking/resolution. Relevant efforts focus both on\n",
      "spreadsheets (TUTA, TABULARNET) and relational\n",
      "tables (TURL, DODUO).\n",
      "\n",
      "Table Content Population (TCP): Unlike\n",
      "TMP, where the table metadata is noisy or miss-\n",
      "ing, TCP deals with corrupted cell content. Given\n",
      "an input table with missing cell values, the ob-\n",
      "jective is to impute the respective values (RPT,\n",
      "TABBIE, TURL).\n",
      "\n",
      "We observe that most tasks can be seen as tradi-\n",
      "tional NLP problems where structured data replace\n",
      "free text, such as the case of QA where answers\n",
      "are located in tabular data instead of documents\n",
      "(Gupta and Gupta, 2012). TFC involves retriev-\n",
      "ing cells that entail or refute a given statement,\n",
      "whereas on free text the corresponding objective\n",
      "is to select sentences as evidence (Thorne et al.,\n",
      "2018). SP is the task of converting natural lan-\n",
      "guage utterance into a logical form (Berant and\n",
      "Liang, 2014), which in this setting is expressed as\n",
      "a declarative query over a relational table. TR on\n",
      "tabular data corresponds to passage retrieval on\n",
      "free text (Kaszkiel and Zobel, 1997). TCP is anal-\n",
      "ogous to predicting missing words or values in a\n",
      "sentence (Devlin et al., 2019). Finally, TMP can be\n",
      "related to syntactic parsing in NLP (Van Gompel\n",
      "and Pickering, 2007), where relationships between\n",
      "different tokens are depicted.\n",
      "\n",
      "We conclude this section with an analysis of\n",
      "the performance of the systems over the different\n",
      "downstream tasks. For every task, we selected\n",
      "datasets for which at least two systems have re-\n",
      "ported results. All datasets reported in Table 5 are\n",
      "described in Table 2, with the exception of WCC\n",
      "\n",
      "237\n",
      "\n",
      "l\n",
      "\n",
      "D\n",
      "o\n",
      "w\n",
      "n\n",
      "o\n",
      "a\n",
      "d\n",
      "e\n",
      "d\n",
      "\n",
      "f\n",
      "r\n",
      "o\n",
      "m\n",
      "h\n",
      "\n",
      "t\n",
      "t\n",
      "\n",
      "p\n",
      "\n",
      ":\n",
      "/\n",
      "/\n",
      "\n",
      "d\n",
      "i\n",
      "r\n",
      "e\n",
      "c\n",
      "t\n",
      ".\n",
      "\n",
      "m\n",
      "\n",
      "i\n",
      "t\n",
      ".\n",
      "\n",
      "e\n",
      "d\n",
      "u\n",
      "\n",
      "/\n",
      "t\n",
      "\n",
      "a\n",
      "c\n",
      "l\n",
      "/\n",
      "\n",
      "l\n",
      "\n",
      "a\n",
      "r\n",
      "t\n",
      "i\n",
      "c\n",
      "e\n",
      "-\n",
      "p\n",
      "d\n",
      "\n",
      "f\n",
      "/\n",
      "\n",
      "d\n",
      "o\n",
      "\n",
      "i\n",
      "/\n",
      "\n",
      ".\n",
      "\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "6\n",
      "2\n",
      "\n",
      "/\n",
      "t\n",
      "\n",
      "l\n",
      "\n",
      "a\n",
      "c\n",
      "_\n",
      "a\n",
      "_\n",
      "0\n",
      "0\n",
      "5\n",
      "4\n",
      "4\n",
      "2\n",
      "0\n",
      "7\n",
      "4\n",
      "8\n",
      "7\n",
      "3\n",
      "\n",
      "/\n",
      "\n",
      "/\n",
      "t\n",
      "\n",
      "l\n",
      "\n",
      "a\n",
      "c\n",
      "_\n",
      "a\n",
      "_\n",
      "0\n",
      "0\n",
      "5\n",
      "4\n",
      "4\n",
      "p\n",
      "d\n",
      "\n",
      ".\n",
      "\n",
      "f\n",
      "\n",
      "b\n",
      "y\n",
      "g\n",
      "u\n",
      "e\n",
      "s\n",
      "t\n",
      "\n",
      "o\n",
      "n\n",
      "2\n",
      "3\n",
      "D\n",
      "e\n",
      "c\n",
      "e\n",
      "m\n",
      "b\n",
      "e\n",
      "r\n",
      "2\n",
      "0\n",
      "2\n",
      "3\n",
      "\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "\fSystem (size)\n",
      "\n",
      "TABERT (367M)\n",
      "TAPAS (340M)\n",
      "MATE (340M)\n",
      "TAPEX (406M)\n",
      "TABBIE (170M)\n",
      "RCI (235M)\n",
      "GRAPPA (355M)\n",
      "DODUO (110M)\n",
      "DECO (1454 M)\n",
      "TUTA (134M)\n",
      "GTR (117M)\n",
      "\n",
      "TFC\n",
      "\n",
      "QA\n",
      "\n",
      "SP\n",
      "\n",
      "TR\n",
      "\n",
      "TMP\n",
      "\n",
      "TCP\n",
      "\n",
      "TabFact HybridQA WikiTQ WikiSQL Spider WQT WCC VizNet EntiTab\n",
      "\n",
      "accu.\n",
      "\n",
      "accu./F1\n",
      "\n",
      "accu.\n",
      "\n",
      "accu.\n",
      "\n",
      "MAP MAP\n",
      "\n",
      "F1\n",
      "\n",
      "F1\n",
      "\n",
      "MAP\n",
      "\n",
      "53.5*\n",
      "48.8\n",
      "51.5\n",
      "57.5\n",
      "\n",
      "52.1*\n",
      "\n",
      "70.9*\n",
      "86.4\n",
      "\n",
      "89.5\n",
      "\n",
      "89.8\n",
      "\n",
      "62.7/70.0\n",
      "62.8/70.2\n",
      "\n",
      "81.2*\n",
      "81.4\n",
      "84.2\n",
      "\n",
      "82.7\n",
      "\n",
      "65.2\n",
      "\n",
      "63.4 83.6*\n",
      "\n",
      "97.2*\n",
      "\n",
      "33.1*\n",
      "\n",
      "96.9\n",
      "\n",
      "37.9\n",
      "\n",
      "94.3\n",
      "\n",
      "69.6\n",
      "\n",
      "87.6\n",
      "\n",
      "73.7\n",
      "\n",
      "Table 5: Performance table of results reported from the original system papers for every downstream\n",
      "task and different datasets. Metrics (accuracy, F1 measure, MAP) change across datasets. Results repro-\n",
      "duced by follow-up papers are denoted with *. Reported sizes are for the largest model of every system.\n",
      "\n",
      "(Ghasemi-Gol and Szekely, 2018), which contains\n",
      "web tables annotated with their type (relational,\n",
      "entity, matrix, list, and non-data), and EntiTab\n",
      "(Zhang and Balog, 2017), which contains web ta-\n",
      "bles annotated with possible header labels for the\n",
      "column population task. Table 5 also contains the\n",
      "size, expressed as number of parameters, of the larg-\n",
      "est model used by every system. As some sys-\n",
      "tems are not comparable on any shared datasets,\n",
      "we report here their size: TABFACT (110M), MMR\n",
      "(87M), TURL (314M), RPT (139M), and CLTR\n",
      "(235M). Larger models do not correlate with\n",
      "better performance across different systems for\n",
      "the same task, but a larger model always brings\n",
      "higher scores for the same system, as expected.\n",
      "Execution times for training and testing de-\n",
      "pend on the size of the model and the comput-\n",
      "ing architecture.\n",
      "\n",
      "The results in the table show that some tasks,\n",
      "such as TFC and TMP, can already be han-\n",
      "dled successfully by the systems, while some\n",
      "tasks, such as TCP, TR, and SP, are harder.\n",
      "QA is the task supported by most systems and\n",
      "the quality of the results vary depending on\n",
      "the dataset at hand. Differences in performance\n",
      "can be explained with different improvements\n",
      "across the systems. For example, MATE has bet-\n",
      "ter performance with respect to TAPAS in two\n",
      "tasks because of its mechanism to deal with\n",
      "larger input tables. Similarly, TUTA improves\n",
      "over TABERT because it handles tabular data\n",
      "\n",
      "beyond relational tables. Finally, TABERT and\n",
      "TAPAS are the systems that show most cover-\n",
      "age in terms of tasks, with multiple papers using\n",
      "them as baselines in their experiments. For the sys-\n",
      "tems that are not reported in Table 5, we notice that\n",
      "TURL obtains similar F1 results for column type\n",
      "prediction, but on a dataset different from VizNet;\n",
      "TURL, TABBIE, and TABERT also report com-\n",
      "parable MAP results for the row population task\n",
      "(not in Table 5) over different datasets.\n",
      "\n",
      "7 Using the Language Model\n",
      "\n",
      "The initial neural representations of tabular data\n",
      "are the result of the pre-training. Systems use the\n",
      "output LM in different ways. It can be fine-tuned\n",
      "or used ‘‘as-is’’—for example, by using its repre-\n",
      "sentations as features in traditional ML algorithms\n",
      "(Section 7.1). However, as pre-trained LMs can\n",
      "act as encoders of the input, they are also used as\n",
      "a module in bigger systems (Section 7.2).\n",
      "\n",
      "7.1 Encoder Output and its Usage\n",
      "\n",
      "the output\n",
      "\n",
      "The alternative systems expose different gran-\n",
      "table representation as\n",
      "ularity of\n",
      "embeddings. Almost all systems provide token\n",
      "and cell output embeddings. Most of them also\n",
      "expose column and row embeddings, while few\n",
      "provide table and pairwise column (DODUO) or\n",
      "pairwise table (DECO) embeddings. When a spe-\n",
      "cial separator token separates the context and the\n",
      "\n",
      "238\n",
      "\n",
      "l\n",
      "\n",
      "D\n",
      "o\n",
      "w\n",
      "n\n",
      "o\n",
      "a\n",
      "d\n",
      "e\n",
      "d\n",
      "\n",
      "f\n",
      "r\n",
      "o\n",
      "m\n",
      "h\n",
      "\n",
      "t\n",
      "t\n",
      "\n",
      "p\n",
      "\n",
      ":\n",
      "/\n",
      "/\n",
      "\n",
      "d\n",
      "i\n",
      "r\n",
      "e\n",
      "c\n",
      "t\n",
      ".\n",
      "\n",
      "m\n",
      "\n",
      "i\n",
      "t\n",
      ".\n",
      "\n",
      "e\n",
      "d\n",
      "u\n",
      "\n",
      "/\n",
      "t\n",
      "\n",
      "a\n",
      "c\n",
      "l\n",
      "/\n",
      "\n",
      "l\n",
      "\n",
      "a\n",
      "r\n",
      "t\n",
      "i\n",
      "c\n",
      "e\n",
      "-\n",
      "p\n",
      "d\n",
      "\n",
      "f\n",
      "/\n",
      "\n",
      "d\n",
      "o\n",
      "\n",
      "i\n",
      "/\n",
      "\n",
      ".\n",
      "\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "6\n",
      "2\n",
      "\n",
      "/\n",
      "t\n",
      "\n",
      "l\n",
      "\n",
      "a\n",
      "c\n",
      "_\n",
      "a\n",
      "_\n",
      "0\n",
      "0\n",
      "5\n",
      "4\n",
      "4\n",
      "2\n",
      "0\n",
      "7\n",
      "4\n",
      "8\n",
      "7\n",
      "3\n",
      "\n",
      "/\n",
      "\n",
      "/\n",
      "t\n",
      "\n",
      "l\n",
      "\n",
      "a\n",
      "c\n",
      "_\n",
      "a\n",
      "_\n",
      "0\n",
      "0\n",
      "5\n",
      "4\n",
      "4\n",
      "p\n",
      "d\n",
      "\n",
      ".\n",
      "\n",
      "f\n",
      "\n",
      "b\n",
      "y\n",
      "g\n",
      "u\n",
      "e\n",
      "s\n",
      "t\n",
      "\n",
      "o\n",
      "n\n",
      "2\n",
      "3\n",
      "D\n",
      "e\n",
      "c\n",
      "e\n",
      "m\n",
      "b\n",
      "e\n",
      "r\n",
      "2\n",
      "0\n",
      "2\n",
      "3\n",
      "\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "\ftable content, a representation of the context is\n",
      "also provided (TABFACT, CLTR, DTR, MMR).\n",
      "\n",
      "These representations can be used in an arbi-\n",
      "trary ML program simply as features or directly\n",
      "by the systems creating them to tackle a given task\n",
      "(Section 6). Indeed, as we discussed in Section 5.2.3,\n",
      "most systems use the pre-trained embeddings for\n",
      "further fine-tuning to tackle the tasks. We observe\n",
      "a relationship between the granularity of the output\n",
      "representations and the target downstream tasks.\n",
      "For instance, table representations are used for the\n",
      "TR task, while column-pairs representations are\n",
      "used for the TMP task to support columns rela-\n",
      "tion annotation. Cell representations are used for\n",
      "the QA task, since the cells including the answer\n",
      "should be returned. Finally, column representa-\n",
      "tions are used for the SP task, as columns are\n",
      "needed to formulate the output query.\n",
      "\n",
      "Most pre-trained models, such as TAPAS, are\n",
      "usually available out-of-the-box, while in some\n",
      "cases, such as in TURL, MATE, and CLTR, the\n",
      "users have to retrain the models. For such systems,\n",
      "the code and the dataset are available, but the users\n",
      "need to run the training on their side to generate\n",
      "the representations (no checkpoints available).\n",
      "\n",
      "7.2 Encoder as a Module\n",
      "\n",
      "Several systems, such as TUTA and TURL, add\n",
      "layers on top of the LMs, which are then fine-tuned\n",
      "for a task. While this is a common use of pre-\n",
      "trained LMs, other works employ LMs as compo-\n",
      "nents in a larger system. In these cases, the system\n",
      "not only needs to learn how to encode properly,\n",
      "but also to adjust its representations to a certain\n",
      "task by training end-to-end together multiple com-\n",
      "ponents, such as the ones that (individually) gen-\n",
      "erate the embeddings for tables and text with the\n",
      "one for scoring similarity of textual and tabular\n",
      "embeddings.\n",
      "\n",
      "Most of these larger systems focus on the re-\n",
      "trieval of tables from an input natural language\n",
      "query. DTR answers a natural language ques-\n",
      "tion from a corpus of tables in two steps. First,\n",
      "it retrieves a small set of candidate tables (TR),\n",
      "where encoding of questions and tables are learned\n",
      "through similarity learning. The similarity score is\n",
      "obtained through an inner product between ques-\n",
      "tions and table embeddings. Then, it performs the\n",
      "standard answer prediction (QA) with each can-\n",
      "didate table in the input. A recent study (Wang\n",
      "et al., 2022) claims that table-specific models may\n",
      "\n",
      "not be needed for accurate table retrieval and that\n",
      "fine-tuning a general text-based retriever leads to\n",
      "superior results compared to DTR. MMR stud-\n",
      "ies a multi-modal version of DTR using both\n",
      "tables and text passages by proposing several\n",
      "bi-encoders and tri-encoders. Similarly, CLTR in-\n",
      "troduces an end-to-end system for QA over a table\n",
      "corpus, where the retrieval of candidate tables\n",
      "is performed by a coarse-grained BM25 module,\n",
      "followed by a transformer-based model that con-\n",
      "catenates the question with the row/column and\n",
      "classifies whether the associated row/column con-\n",
      "tains the answer. Other systems, as GTR, support\n",
      "retrieval of tables, where tables are represented by\n",
      "graphs. In this setting, stacked layers of a variant\n",
      "of Graph Transformers (Koncel-Kedziorski et al.,\n",
      "2019) are employed for obtaining node features\n",
      "that are combined with query embeddings. These\n",
      "combined embeddings are then aggregated with\n",
      "the BERT embeddings of the table context and\n",
      "query, and a relevance score is finally obtained.\n",
      "\n",
      "8 Future Directions\n",
      "\n",
      "Tabular LMs effectively address some of the\n",
      "challenges that arise with classical ML models\n",
      "(Borisov et al., 2021), such as transfer learning\n",
      "and self-supervision. However, several challenges\n",
      "remain unaddressed.\n",
      "\n",
      "Interpretability. Only a few systems expose a\n",
      "justification of their model output, for example,\n",
      "TAPAS, CLTR, and MATE,\n",
      "thus model usage\n",
      "remains a black box. One direction is to use\n",
      "the attention mechanism to derive interpretations\n",
      "(Serrano and Smith, 2019; Dong et al., 2021).\n",
      "Looking at self-attention weights of particular lay-\n",
      "ers and layer-wise propagation with respect to the\n",
      "input tokens, we can capture the influence of each\n",
      "through back-\n",
      "cell value/tuple on the output\n",
      "propagation (Huang et al., 2019). For instance,\n",
      "in TFC, providing explanation is crucial when the\n",
      "decision is derived from aggregating several cells,\n",
      "such as sum or average operation. In this case, a\n",
      "basic explanation would be to show all the cells\n",
      "that led to the final true/false decision.\n",
      "\n",
      "Error Analysis. Most studies focus on the\n",
      "downstream evaluation scores rather than going\n",
      "through manual evaluation of errors. In the down-\n",
      "stream task level, this analysis could trace back\n",
      "misclassified examples to get evidence of issues\n",
      "in the tabular data representation. For example,\n",
      "\n",
      "239\n",
      "\n",
      "l\n",
      "\n",
      "D\n",
      "o\n",
      "w\n",
      "n\n",
      "o\n",
      "a\n",
      "d\n",
      "e\n",
      "d\n",
      "\n",
      "f\n",
      "r\n",
      "o\n",
      "m\n",
      "h\n",
      "\n",
      "t\n",
      "t\n",
      "\n",
      "p\n",
      "\n",
      ":\n",
      "/\n",
      "/\n",
      "\n",
      "d\n",
      "i\n",
      "r\n",
      "e\n",
      "c\n",
      "t\n",
      ".\n",
      "\n",
      "m\n",
      "\n",
      "i\n",
      "t\n",
      ".\n",
      "\n",
      "e\n",
      "d\n",
      "u\n",
      "\n",
      "/\n",
      "t\n",
      "\n",
      "a\n",
      "c\n",
      "l\n",
      "/\n",
      "\n",
      "l\n",
      "\n",
      "a\n",
      "r\n",
      "t\n",
      "i\n",
      "c\n",
      "e\n",
      "-\n",
      "p\n",
      "d\n",
      "\n",
      "f\n",
      "/\n",
      "\n",
      "d\n",
      "o\n",
      "\n",
      "i\n",
      "/\n",
      "\n",
      ".\n",
      "\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "6\n",
      "2\n",
      "\n",
      "/\n",
      "t\n",
      "\n",
      "l\n",
      "\n",
      "a\n",
      "c\n",
      "_\n",
      "a\n",
      "_\n",
      "0\n",
      "0\n",
      "5\n",
      "4\n",
      "4\n",
      "2\n",
      "0\n",
      "7\n",
      "4\n",
      "8\n",
      "7\n",
      "3\n",
      "\n",
      "/\n",
      "\n",
      "/\n",
      "t\n",
      "\n",
      "l\n",
      "\n",
      "a\n",
      "c\n",
      "_\n",
      "a\n",
      "_\n",
      "0\n",
      "0\n",
      "5\n",
      "4\n",
      "4\n",
      "p\n",
      "d\n",
      "\n",
      ".\n",
      "\n",
      "f\n",
      "\n",
      "b\n",
      "y\n",
      "g\n",
      "u\n",
      "e\n",
      "s\n",
      "t\n",
      "\n",
      "o\n",
      "n\n",
      "2\n",
      "3\n",
      "D\n",
      "e\n",
      "c\n",
      "e\n",
      "m\n",
      "b\n",
      "e\n",
      "r\n",
      "2\n",
      "0\n",
      "2\n",
      "3\n",
      "\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "\fin a QA task with wrong answers, there could be\n",
      "a pattern that explains how these errors are all\n",
      "due to the system confusing two columns with\n",
      "similar meaning (e.g., max value and last value\n",
      "for a stock), thus returning the wrong cell value in\n",
      "several cases.\n",
      "\n",
      "Complex Queries and Rich Tables. Several\n",
      "systems, such as TAPEX and MATE, handle\n",
      "queries with aggregations by adding classifica-\n",
      "tion layers. However, such methods fail short with\n",
      "queries that join tables. As most works assume\n",
      "a single table as input, a much needed direction\n",
      "is to develop models that handle multiple tables,\n",
      "for example, with classification layers predicting\n",
      "when a join is required. Also, as tables might\n",
      "contain heterogeneous types and content, such\n",
      "as non-uniform units (e.g., kg and lbs), systems\n",
      "should be able to handle such differences, which\n",
      "are abundant in practice. Moreover, an interesting\n",
      "direction is to conduct a study to show where tab-\n",
      "ular LMs can be successfully applied and where\n",
      "they fail in querying data.\n",
      "\n",
      "Efficiency. Transformer-based\n",
      "\n",
      "Model\n",
      "ap-\n",
      "proaches are computationally expensive and\n",
      "some approaches try to approximate the costly\n",
      "attention mechanism by using locality-sensitive\n",
      "hashing to replace it (Kitaev et al., 2020), approxi-\n",
      "mating it by a low-rank matrix (Wang et al.,\n",
      "2020), or applying kernels to avoid its compu-\n",
      "tational complexity (Katharopoulos et al., 2020;\n",
      "Choromanski et al., 2020). While there exist\n",
      "methods to make transformers more efficient for\n",
      "long context (Tay et al., 2020), all optimizations\n",
      "consider an unstructured textual input. We believe\n",
      "more traction is needed for efficient transformers\n",
      "on structured data, with ideas from the textual\n",
      "counterpart, such as limiting attention heads to\n",
      "rows/columns, which can be obtained naturally\n",
      "from the structured input (Eisenschlos et al.,\n",
      "2021), or exploring prompt learning and delta\n",
      "tuning (Ding et al., 2022).\n",
      "\n",
      "those for word embeddings (Bakarov, 2018), can\n",
      "include predicting table caption given table repre-\n",
      "sentation or identifying functional dependencies.\n",
      "A set of precise tests can be designed to assess\n",
      "data-specific properties, such as the ability of the\n",
      "transformer-based models, designed to model se-\n",
      "quences, to capture that row and attributes are\n",
      "sets in tables. Also, it is not clear whether the\n",
      "model representations are consistent with the table\n",
      "structure, effectively capturing their relationships.\n",
      "For example, given two cell values in the same\n",
      "row/column, are their embeddings closer than val-\n",
      "ues coming from different rows/columns? For\n",
      "this, following the lines of CheckList for text\n",
      "LMs (Ribeiro et al., 2020), basic tests should be\n",
      "designed to measure the consistency of the data\n",
      "representation.\n",
      "\n",
      "Data Bias.\n",
      "It is recognized that LMs incorporate\n",
      "bias in the model parameters in terms of stereo-\n",
      "types, race, and gender (Nadeem et al., 2021; Vig\n",
      "et al., 2020). The bias is implicitly derived from the\n",
      "training corpora used to develop LMs. Therefore,\n",
      "there is a need to develop methods to overcome\n",
      "this drawback by, for instance, pre-filtering the\n",
      "training data or by correcting the tabular LMs,\n",
      "similarly to the ongoing efforts for text LMs (Liu\n",
      "et al., 2021b; Bordia and Bowman, 2019).\n",
      "\n",
      "Green LMs. The use of large-scale transformers\n",
      "for learning LMs requires considerable com-\n",
      "putation, which contributes to global warming\n",
      "(Strubell et al., 2020; Schwartz et al., 2020).\n",
      "Therefore, it is important to consider enhance-\n",
      "ments or potentially new techniques that limit the\n",
      "carbon footprint of tabular language models with-\n",
      "out a significant decrease in the performance in the\n",
      "downstream tasks. One enhancement can be at the\n",
      "level of the size of the training data by removing\n",
      "redundant, or less informative, tuples and tables.\n",
      "How to identify such data is a key challenge.\n",
      "\n",
      "9 Conclusion\n",
      "\n",
      "Benchmarking Data Representations. There\n",
      "are no common benchmark datasets where re-\n",
      "searchers can assess and compare the quality\n",
      "of their data representations in a level playing\n",
      "field. Current evaluation is extrinsic, that is, at\n",
      "the downstream task level, where each work has\n",
      "its own assumptions. Intrinsic methods to eval-\n",
      "uate the quality of the representations, such as\n",
      "\n",
      "We conducted a survey on the efforts in develop-\n",
      "ing transformer-based representations for tabular\n",
      "data. We introduced a high level framework to\n",
      "categorize those efforts and characterized each\n",
      "step in terms of solutions to model structured\n",
      "data, with special attention to the extensions to the\n",
      "transformer architecture. As future work, we envi-\n",
      "sion a generic system to perform an experimental\n",
      "\n",
      "240\n",
      "\n",
      "l\n",
      "\n",
      "D\n",
      "o\n",
      "w\n",
      "n\n",
      "o\n",
      "a\n",
      "d\n",
      "e\n",
      "d\n",
      "\n",
      "f\n",
      "r\n",
      "o\n",
      "m\n",
      "h\n",
      "\n",
      "t\n",
      "t\n",
      "\n",
      "p\n",
      "\n",
      ":\n",
      "/\n",
      "/\n",
      "\n",
      "d\n",
      "i\n",
      "r\n",
      "e\n",
      "c\n",
      "t\n",
      ".\n",
      "\n",
      "m\n",
      "\n",
      "i\n",
      "t\n",
      ".\n",
      "\n",
      "e\n",
      "d\n",
      "u\n",
      "\n",
      "/\n",
      "t\n",
      "\n",
      "a\n",
      "c\n",
      "l\n",
      "/\n",
      "\n",
      "l\n",
      "\n",
      "a\n",
      "r\n",
      "t\n",
      "i\n",
      "c\n",
      "e\n",
      "-\n",
      "p\n",
      "d\n",
      "\n",
      "f\n",
      "/\n",
      "\n",
      "d\n",
      "o\n",
      "\n",
      "i\n",
      "/\n",
      "\n",
      ".\n",
      "\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "6\n",
      "2\n",
      "\n",
      "/\n",
      "t\n",
      "\n",
      "l\n",
      "\n",
      "a\n",
      "c\n",
      "_\n",
      "a\n",
      "_\n",
      "0\n",
      "0\n",
      "5\n",
      "4\n",
      "4\n",
      "2\n",
      "0\n",
      "7\n",
      "4\n",
      "8\n",
      "7\n",
      "3\n",
      "\n",
      "/\n",
      "\n",
      "/\n",
      "t\n",
      "\n",
      "l\n",
      "\n",
      "a\n",
      "c\n",
      "_\n",
      "a\n",
      "_\n",
      "0\n",
      "0\n",
      "5\n",
      "4\n",
      "4\n",
      "p\n",
      "d\n",
      "\n",
      ".\n",
      "\n",
      "f\n",
      "\n",
      "b\n",
      "y\n",
      "g\n",
      "u\n",
      "e\n",
      "s\n",
      "t\n",
      "\n",
      "o\n",
      "n\n",
      "2\n",
      "3\n",
      "D\n",
      "e\n",
      "c\n",
      "e\n",
      "m\n",
      "b\n",
      "e\n",
      "r\n",
      "2\n",
      "0\n",
      "2\n",
      "3\n",
      "\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "\fstudy based on our framework. The first part of the\n",
      "system would develop tabular data representations\n",
      "with alternative design choices, while the second\n",
      "part would evaluate them in downstream tasks.\n",
      "This work would help identifying the impact of\n",
      "alternative techniques on the performance in the\n",
      "final applications.\n",
      "\n",
      "Acknowledgments\n",
      "\n",
      "We would like to thank the action editor and the\n",
      "reviewers for their valuable inputs that helped\n",
      "in further improving the content and the presen-\n",
      "tation of our article. This work has been par-\n",
      "tially supported by the ANR project ATTENTION\n",
      "(ANR-21-CE23-0037) and by gifts from Google.\n",
      "\n",
      "References\n",
      "\n",
      "Rami Aly, Zhijiang Guo, Michael\n",
      "\n",
      "Sejr\n",
      "Schlichtkrull, James Thorne, Andreas Vlachos,\n",
      "Christos Christodoulopoulos, Oana Cocarascu,\n",
      "and Arpit Mittal. 2021. FEVEROUS: Fact\n",
      "extraction and VERification over unstruc-\n",
      "tured and structured information. In Thirty-fifth\n",
      "Conference on Neural Information Process-\n",
      "ing Systems Datasets and Benchmarks Track\n",
      "(Round 1). https://doi.org/10.18653\n",
      "/v1/2021.fever-1.1\n",
      "\n",
      "Wissam Antoun, Fady Baly, and Hazem Hajj.\n",
      "2020. AraBERT: Transformer-based model for\n",
      "arabic language understanding. In Proceedings\n",
      "of\n",
      "the 4th Workshop on Open-Source Ara-\n",
      "bic Corpora and Processing Tools, with a\n",
      "Shared Task on Offensive Language Detection,\n",
      "pages 9–15.\n",
      "\n",
      "Gilbert Badaro, Ramy Baly, Hazem Hajj, Wassim\n",
      "El-Hajj, Khaled Bashir Shaban, Nizar Habash,\n",
      "Ahmad Al-Sallab, and Ali Hamdi. 2019. A\n",
      "survey of opinion mining in Arabic: A compre-\n",
      "hensive system perspective covering challenges\n",
      "and advances in tools, resources, models, appli-\n",
      "cations, and visualizations. ACM Transactions\n",
      "on Asian and Low-Resource Language In-\n",
      "formation Processing (TALLIP), 18(3):1–52.\n",
      "https://doi.org/10.1145/3295662\n",
      "\n",
      "2014 Workshop on Arabic Natural Language\n",
      "Processing (ANLP), pages 165–173. https://\n",
      "doi.org/10.3115/v1/W14-3623\n",
      "\n",
      "Gilbert Badaro, Obeida El Jundi, Alaa Khaddaj,\n",
      "Alaa Maarouf, Raslan Kain, Hazem Hajj, and\n",
      "Wassim El-Hajj. 2018a. EMA at SemEval-\n",
      "2018 task 1: Emotion mining for Arabic. In\n",
      "Proceedings of The 12th International Work-\n",
      "shop on Semantic Evaluation, pages 236–244.\n",
      "https://doi.org/10.18653/v1/S18\n",
      "-1036\n",
      "\n",
      "Gilbert Badaro, Hazem Hajj, and Nizar Habash.\n",
      "2020. A link prediction approach for accurately\n",
      "mapping a large-scale Arabic lexical resource\n",
      "to english wordnet. ACM Transactions on Asian\n",
      "and Low-Resource Language Information Pro-\n",
      "cessing (TALLIP), 19(6):1–38. https://doi\n",
      ".org/10.1145/3404854\n",
      "\n",
      "Gilbert Badaro, Hussein Jundi, Hazem Hajj, and\n",
      "Wassim El-Hajj. 2018b. EmoWordNet: Au-\n",
      "tomatic expansion of emotion lexicon using\n",
      "English Wordnet. In Proceedings of the Seventh\n",
      "Joint Conference on Lexical and Computational\n",
      "Semantics, pages 86–93. https://doi.org\n",
      "/10.18653/v1/S18-2009\n",
      "\n",
      "Gilbert Badaro, Hussein Jundi, Hazem Hajj,\n",
      "Wassim El-Hajj, and Nizar Habash. 2018c.\n",
      "ArSEL: A large scale Arabic sentiment and\n",
      "emotion lexicon. OSACT, 326.\n",
      "\n",
      "for\n",
      "\n",
      "and Paolo Papotti.\n",
      "\n",
      "2022.\n",
      "Gilbert Badaro\n",
      "Transformers\n",
      "tabular data representa-\n",
      "tion: A tutorial on models and applica-\n",
      "tions. Proceedings of the VLDB Endowment,\n",
      "15(12):3746–3749. https://doi.org/10\n",
      ".14778/3554821.3554890\n",
      "\n",
      "Amir Bakarov. 2018. A survey of word em-\n",
      "beddings evaluation methods. arXiv preprint\n",
      "arXiv:1801.09536v1.\n",
      "\n",
      "Jonathan Berant and Percy Liang. 2014. Seman-\n",
      "tic parsing via paraphrasing. In Proceedings\n",
      "of the 52nd Annual Meeting of the Associa-\n",
      "tion for Computational Linguistics (Volume 1:\n",
      "Long Papers), pages 1415–1425. https://\n",
      "doi.org/10.3115/v1/P14-1133\n",
      "\n",
      "Gilbert Badaro, Ramy Baly, Hazem Hajj, Nizar\n",
      "Habash, and Wassim El-Hajj. 2014. A large\n",
      "scale Arabic sentiment lexicon for arabic opin-\n",
      "the EMNLP\n",
      "ion mining. In Proceedings of\n",
      "\n",
      "Chandra Sekhar Bhagavatula, Thanapon Noraset,\n",
      "and Doug Downey. 2015. TabEL: Entity link-\n",
      "ing in web tables. In International Semantic\n",
      "Web Conference, pages 425–441. Springer.\n",
      "\n",
      "241\n",
      "\n",
      "l\n",
      "\n",
      "D\n",
      "o\n",
      "w\n",
      "n\n",
      "o\n",
      "a\n",
      "d\n",
      "e\n",
      "d\n",
      "\n",
      "f\n",
      "r\n",
      "o\n",
      "m\n",
      "h\n",
      "\n",
      "t\n",
      "t\n",
      "\n",
      "p\n",
      "\n",
      ":\n",
      "/\n",
      "/\n",
      "\n",
      "d\n",
      "i\n",
      "r\n",
      "e\n",
      "c\n",
      "t\n",
      ".\n",
      "\n",
      "m\n",
      "\n",
      "i\n",
      "t\n",
      ".\n",
      "\n",
      "e\n",
      "d\n",
      "u\n",
      "\n",
      "/\n",
      "t\n",
      "\n",
      "a\n",
      "c\n",
      "l\n",
      "/\n",
      "\n",
      "l\n",
      "\n",
      "a\n",
      "r\n",
      "t\n",
      "i\n",
      "c\n",
      "e\n",
      "-\n",
      "p\n",
      "d\n",
      "\n",
      "f\n",
      "/\n",
      "\n",
      "d\n",
      "o\n",
      "\n",
      "i\n",
      "/\n",
      "\n",
      ".\n",
      "\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "6\n",
      "2\n",
      "\n",
      "/\n",
      "t\n",
      "\n",
      "l\n",
      "\n",
      "a\n",
      "c\n",
      "_\n",
      "a\n",
      "_\n",
      "0\n",
      "0\n",
      "5\n",
      "4\n",
      "4\n",
      "2\n",
      "0\n",
      "7\n",
      "4\n",
      "8\n",
      "7\n",
      "3\n",
      "\n",
      "/\n",
      "\n",
      "/\n",
      "t\n",
      "\n",
      "l\n",
      "\n",
      "a\n",
      "c\n",
      "_\n",
      "a\n",
      "_\n",
      "0\n",
      "0\n",
      "5\n",
      "4\n",
      "4\n",
      "p\n",
      "d\n",
      "\n",
      ".\n",
      "\n",
      "f\n",
      "\n",
      "b\n",
      "y\n",
      "g\n",
      "u\n",
      "e\n",
      "s\n",
      "t\n",
      "\n",
      "o\n",
      "n\n",
      "2\n",
      "3\n",
      "D\n",
      "e\n",
      "c\n",
      "e\n",
      "m\n",
      "b\n",
      "e\n",
      "r\n",
      "2\n",
      "0\n",
      "2\n",
      "3\n",
      "\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "\fhttps://doi.org/10.1007/978-3-319\n",
      "-25007-6 25\n",
      "\n",
      "large-scale retrieval. In International Confer-\n",
      "ence on Learning Representations.\n",
      "\n",
      "Shikha Bordia and Samuel R. Bowman. 2019.\n",
      "Identifying and reducing gender bias\n",
      "in\n",
      "language models. In 2019 Con-\n",
      "word-level\n",
      "ference of\n",
      "the North American Chapter of\n",
      "the Association for Computational Linguistics:\n",
      "Human Language Technologies, NAACL HLT\n",
      "2019-Student Research Workshop, SRW 2019,\n",
      "pages 7–15. Association for Computational\n",
      "Linguistics (ACL). https://doi.org/10\n",
      ".18653/v1/N19-3002\n",
      "\n",
      "Vadim Borisov, Tobias Leemann, Kathrin\n",
      "Seßler, Johannes Haug, Martin Pawelczyk,\n",
      "and Gjergji Kasneci. 2021. Deep neural net-\n",
      "works and tabular data: A survey. arXiv\n",
      "preprint arXiv:2110.01889v3. https://doi\n",
      ".org/10.1109/TNNLS.2022.3229161\n",
      "\n",
      "Tom B. Brown, Benjamin Mann, Nick Ryder,\n",
      "Melanie Subbiah,\n",
      "Jared Kaplan, Prafulla\n",
      "Dhariwal, Arvind Neelakantan, Pranav Shyam,\n",
      "Girish Sastry, Amanda Askell, Sandhini Agarwal,\n",
      "Ariel Herbert-Voss, Gretchen Krueger, Tom\n",
      "Henighan, Rewon Child, Aditya Ramesh,\n",
      "Daniel M. Ziegler, Jeffrey Wu, Clemens Winter,\n",
      "Christopher Hesse, Mark Chen, Eric Sigler,\n",
      "Mateusz Litwin, Scott Gray, Benjamin Chess,\n",
      "Jack Clark, Christopher Berner, Sam McCandlish,\n",
      "Alec Radford,\n",
      "Ilya Sutskever, and Dario\n",
      "Amodei. 2020. Language models are few-shot\n",
      "learners. CoRR, abs/2005.14165.\n",
      "\n",
      "Riccardo Cappuzzo, Paolo Papotti, and Saravanan\n",
      "Thirumuruganathan. 2020. Creating embed-\n",
      "dings of heterogeneous relational datasets for\n",
      "data integration tasks. In Proceedings of the\n",
      "2020 ACM SIGMOD International Confer-\n",
      "ence on Management of Data, SIGMOD ’20,\n",
      "pages 1335–1349, New York, NY, USA. Asso-\n",
      "ciation for Computing Machinery. https://\n",
      "doi.org/10.1145/3318464.3389742\n",
      "\n",
      "Yoan Chabot, Pierre Monnin, Fr´ed´eric Deuz´e,\n",
      "Viet-Phi Huynh, Thomas Labb´e, Jixiong Liu,\n",
      "and Rapha¨el Troncy. 2021. A framework for\n",
      "automatically interpreting tabular data at or-\n",
      "ange. In Proceedings of the 20th International\n",
      "Semantic Web Conference.\n",
      "\n",
      "Wei-Cheng Chang, Felix X. Yu, Yin-Wen\n",
      "Chang, Yiming Yang, and Sanjiv Kumar.\n",
      "2020. Pre-training tasks for embedding-based\n",
      "\n",
      "Wenhu Chen, Ming-Wei Chang, Eva Schlinger,\n",
      "William Yang Wang, and William W. Cohen.\n",
      "2021. Open question answering over tables and\n",
      "text. In International Conference on Learning\n",
      "Representations.\n",
      "\n",
      "Wenhu Chen, Jianshu Chen, Yu Su, Zhiyu Chen,\n",
      "and William Yang Wang. 2020a. Logical nat-\n",
      "language generation from open-domain\n",
      "ural\n",
      "tables. In Proceedings of\n",
      "the 58th Annual\n",
      "Meeting of the Association for Computational\n",
      "Linguistics, pages 7929–7942. https://doi\n",
      ".org/10.18653/v1/2020.acl-main.708\n",
      "\n",
      "Wenhu Chen, Hongmin Wang, Jianshu Chen,\n",
      "Yunkai Zhang, Hong Wang, Shiyang Li, Xiyou\n",
      "Zhou, and William Yang Wang. 2020b. Tab-\n",
      "Fact: A large-scale dataset for table-based fact\n",
      "verification. In International Conference on\n",
      "Learning Representations.\n",
      "\n",
      "Wenhu Chen, Hanwen Zha, Zhiyu Chen, Wenhan\n",
      "Xiong, Hong Wang, and William Yang Wang.\n",
      "2020c. HybridQA: A dataset of multi-hop ques-\n",
      "tion answering over tabular and textual data. In\n",
      "Findings of the Association for Computational\n",
      "Linguistics: EMNLP 2020, pages 1026–1036,\n",
      "Online. Association for Computational Linguis-\n",
      "tics. https://doi.org/10.18653/v1\n",
      "/2020.findings-emnlp.91\n",
      "\n",
      "Zhiyu Chen, Wenhu Chen, Hanwen Zha, Xiyou\n",
      "Zhou, Yunkai Zhang, Sairam Sundaresan,\n",
      "and William Yang Wang. 2020d. Logic2Text:\n",
      "High-fidelity natural language generation from\n",
      "logical forms. In Findings of the Association\n",
      "for Computational Linguistics: EMNLP 2020,\n",
      "pages 2096–2111. https://doi.org/10\n",
      ".18653/v1/2020.findings-emnlp.190\n",
      "\n",
      "Zhiyu Chen, Harini Eavani, Wenhu Chen, Yinyin\n",
      "Liu, and William Yang Wang. 2020e. Few-shot\n",
      "nlg with pre-trained language model. In Pro-\n",
      "ceedings of\n",
      "the 58th Annual Meeting of\n",
      "the Association for Computational Linguis-\n",
      "tics, pages 183–190. https://doi.org\n",
      "/10.18653/v1/2020.acl-main.18\n",
      "\n",
      "Zhoujun Cheng, Haoyu Dong, Ran Jia, Pengfei\n",
      "Wu, Shi Han, Fan Cheng, and Dongmei\n",
      "Zhang. 2022. FORTAP: Using formulas for\n",
      "numerical-reasoning-aware table pretraining.\n",
      "\n",
      "242\n",
      "\n",
      "l\n",
      "\n",
      "D\n",
      "o\n",
      "w\n",
      "n\n",
      "o\n",
      "a\n",
      "d\n",
      "e\n",
      "d\n",
      "\n",
      "f\n",
      "r\n",
      "o\n",
      "m\n",
      "h\n",
      "\n",
      "t\n",
      "t\n",
      "\n",
      "p\n",
      "\n",
      ":\n",
      "/\n",
      "/\n",
      "\n",
      "d\n",
      "i\n",
      "r\n",
      "e\n",
      "c\n",
      "t\n",
      ".\n",
      "\n",
      "m\n",
      "\n",
      "i\n",
      "t\n",
      ".\n",
      "\n",
      "e\n",
      "d\n",
      "u\n",
      "\n",
      "/\n",
      "t\n",
      "\n",
      "a\n",
      "c\n",
      "l\n",
      "/\n",
      "\n",
      "l\n",
      "\n",
      "a\n",
      "r\n",
      "t\n",
      "i\n",
      "c\n",
      "e\n",
      "-\n",
      "p\n",
      "d\n",
      "\n",
      "f\n",
      "/\n",
      "\n",
      "d\n",
      "o\n",
      "\n",
      "i\n",
      "/\n",
      "\n",
      ".\n",
      "\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "6\n",
      "2\n",
      "\n",
      "/\n",
      "t\n",
      "\n",
      "l\n",
      "\n",
      "a\n",
      "c\n",
      "_\n",
      "a\n",
      "_\n",
      "0\n",
      "0\n",
      "5\n",
      "4\n",
      "4\n",
      "2\n",
      "0\n",
      "7\n",
      "4\n",
      "8\n",
      "7\n",
      "3\n",
      "\n",
      "/\n",
      "\n",
      "/\n",
      "t\n",
      "\n",
      "l\n",
      "\n",
      "a\n",
      "c\n",
      "_\n",
      "a\n",
      "_\n",
      "0\n",
      "0\n",
      "5\n",
      "4\n",
      "4\n",
      "p\n",
      "d\n",
      "\n",
      ".\n",
      "\n",
      "f\n",
      "\n",
      "b\n",
      "y\n",
      "g\n",
      "u\n",
      "e\n",
      "s\n",
      "t\n",
      "\n",
      "o\n",
      "n\n",
      "2\n",
      "3\n",
      "D\n",
      "e\n",
      "c\n",
      "e\n",
      "m\n",
      "b\n",
      "e\n",
      "r\n",
      "2\n",
      "0\n",
      "2\n",
      "3\n",
      "\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "\fIn Proceedings of the 60th Annual Meeting of\n",
      "the Association for Computational Linguistics\n",
      "(Volume 1: Long Papers), pages 1150–1166.\n",
      "https://doi.org/10.18653/v1/2022\n",
      ".acl-long.82\n",
      "\n",
      "Radostin Cholakov and Todor Kolev. 2021. Trans-\n",
      "formers predicting the future. Applying atten-\n",
      "tion in next-frame and time series forecasting.\n",
      "arXiv preprint arXiv:2108.08224v1.\n",
      "\n",
      "Krzysztof Choromanski, Valerii Likhosherstov,\n",
      "David Dohan, Xingyou Song, Andreea Gane,\n",
      "Tamas Sarlos, Peter Hawkins, Jared Davis,\n",
      "David Belanger, Lucy Colwell, and Adrian\n",
      "Weller. 2020. Masked language modeling for\n",
      "proteins via linearly scalable long-context trans-\n",
      "formers. arXiv preprint arXiv:2006.03555v3.\n",
      "\n",
      "Kevin Clark, Minh-Thang Luong, Quoc V. Le,\n",
      "and Christopher D. Manning. 2020. ELECTRA:\n",
      "pre-training text encoders as discriminators\n",
      "rather than generators. In 8th International\n",
      "Conference on Learning Representations, ICLR\n",
      "2020, Addis Ababa, Ethiopia, April 26–30,\n",
      "2020. OpenReview.net.\n",
      "\n",
      "Ido Dagan, Dan Roth, Mark Sammons, and\n",
      "Fabio Massimo Zanzotto. 2013. Recognizing\n",
      "Textual Entailment: Models and Applications.\n",
      "Synthesis Lectures on Human Language Tech-\n",
      "nologies. Morgan and Claypool publishers.\n",
      "https://doi.org/10.1007/978-3-031\n",
      "-02151-0\n",
      "\n",
      "Sabyasachi Dash, Sushil Kumar Shakyawar,\n",
      "Mohit Sharma, and Sandeep Kaushik. 2019.\n",
      "Big data in healthcare: Management, analy-\n",
      "sis and future prospects. Journal of Big Data,\n",
      "6(1):1–25. https://doi.org/10.1186\n",
      "/s40537-019-0217-0\n",
      "\n",
      "Xiang Deng, Huan Sun, Alyssa Lees, You\n",
      "Wu, and Cong Yu. 2020. TURL: Table\n",
      "understanding through representation learn-\n",
      "the VLDB Endow-\n",
      "ing. Proceedings of\n",
      "ment, 14(3):307–319. https://doi.org\n",
      "/10.14778/3430915.3430921\n",
      "\n",
      "(Long and Short Papers), pages 4171–4186,\n",
      "Minneapolis, Minnesota. Association for Com-\n",
      "putational Linguistics.\n",
      "\n",
      "Ning Ding, Yujia Qin, Guang Yang, Fuchao\n",
      "Wei, Zonghan Yang, Yusheng Su, Shengding\n",
      "Hu, Yulin Chen, Chi-Min Chan, Weize Chen,\n",
      "et al. 2022. Delta tuning: A comprehen-\n",
      "sive study of parameter efficient methods for\n",
      "pre-trained language models. arXiv preprint\n",
      "https://doi.org\n",
      "arXiv:2203.06904v2.\n",
      "/10.21203/rs.3.rs-1553541/v1\n",
      "\n",
      "Haoyu Dong, Zhoujun Cheng, Xinyi He, Mengyu\n",
      "Zhou, Anda Zhou, Fan Zhou, Ao Liu, Shi Han,\n",
      "and Dongmei Zhang. 2022. Table pre-training:\n",
      "A survey on model architectures, pre-training\n",
      "objectives, and downstream tasks. In Proceed-\n",
      "ings of the Thirty-First International Joint Con-\n",
      "ference on Artificial Intelligence, IJCAI-22,\n",
      "pages 5426–5435. International Joint Confer-\n",
      "ences on Artificial Intelligence Organization.\n",
      "Survey Track. https://doi.org/10.24963\n",
      "/ijcai.2022/761\n",
      "\n",
      "Haoyu Dong, Shijie Liu, Zhouyu Fu, Shi Han,\n",
      "and Dongmei Zhang. 2019. Semantic struc-\n",
      "ture extraction for spreadsheet tables with a\n",
      "multi-task learning architecture. In Workshop\n",
      "on Document Intelligence at NeurIPS 2019.\n",
      "\n",
      "Yihe Dong,\n",
      "\n",
      "Jean-Baptiste Cordonnier,\n",
      "\n",
      "and\n",
      "Andreas Loukas. 2021. Attention is not all you\n",
      "need: Pure attention loses rank doubly expo-\n",
      "nentially with depth. In International Confer-\n",
      "ence on Machine Learning, pages 2793–2803.\n",
      "PMLR.\n",
      "\n",
      "Alexey Dosovitskiy, Lucas Beyer, Alexander\n",
      "Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,\n",
      "Thomas Unterthiner, Mostafa Dehghani,\n",
      "Matthias Minderer, Georg Heigold, Sylvain\n",
      "Gelly, Jakob Uszkoreit, and Neil Houlsby.\n",
      "2020. An image is worth 16x16 words: Trans-\n",
      "formers for image recognition at scale. In\n",
      "International Conference on Learning Repre-\n",
      "sentations.\n",
      "\n",
      "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and\n",
      "Kristina Toutanova. 2019. BERT: Pre-training\n",
      "of deep bidirectional transformers for language\n",
      "understanding. In Proceedings of\n",
      "the 2019\n",
      "Conference of the North American Chapter\n",
      "of the Association for Computational Linguis-\n",
      "tics: Human Language Technologies, Volume 1\n",
      "\n",
      "Lun Du, Fei Gao, Xu Chen, Ran Jia, Junshan\n",
      "Wang, Jiang Zhang, Shi Han, and Dongmei\n",
      "Zhang. 2021. TabularNet: A neural network\n",
      "architecture for understanding semantic struc-\n",
      "tures of tabular data. In Proceedings of the\n",
      "27th ACM SIGKDD Conference on Knowledge\n",
      "Discovery & Data Mining, pages 322–331.\n",
      "\n",
      "243\n",
      "\n",
      "l\n",
      "\n",
      "D\n",
      "o\n",
      "w\n",
      "n\n",
      "o\n",
      "a\n",
      "d\n",
      "e\n",
      "d\n",
      "\n",
      "f\n",
      "r\n",
      "o\n",
      "m\n",
      "h\n",
      "\n",
      "t\n",
      "t\n",
      "\n",
      "p\n",
      "\n",
      ":\n",
      "/\n",
      "/\n",
      "\n",
      "d\n",
      "i\n",
      "r\n",
      "e\n",
      "c\n",
      "t\n",
      ".\n",
      "\n",
      "m\n",
      "\n",
      "i\n",
      "t\n",
      ".\n",
      "\n",
      "e\n",
      "d\n",
      "u\n",
      "\n",
      "/\n",
      "t\n",
      "\n",
      "a\n",
      "c\n",
      "l\n",
      "/\n",
      "\n",
      "l\n",
      "\n",
      "a\n",
      "r\n",
      "t\n",
      "i\n",
      "c\n",
      "e\n",
      "-\n",
      "p\n",
      "d\n",
      "\n",
      "f\n",
      "/\n",
      "\n",
      "d\n",
      "o\n",
      "\n",
      "i\n",
      "/\n",
      "\n",
      ".\n",
      "\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "6\n",
      "2\n",
      "\n",
      "/\n",
      "t\n",
      "\n",
      "l\n",
      "\n",
      "a\n",
      "c\n",
      "_\n",
      "a\n",
      "_\n",
      "0\n",
      "0\n",
      "5\n",
      "4\n",
      "4\n",
      "2\n",
      "0\n",
      "7\n",
      "4\n",
      "8\n",
      "7\n",
      "3\n",
      "\n",
      "/\n",
      "\n",
      "/\n",
      "t\n",
      "\n",
      "l\n",
      "\n",
      "a\n",
      "c\n",
      "_\n",
      "a\n",
      "_\n",
      "0\n",
      "0\n",
      "5\n",
      "4\n",
      "4\n",
      "p\n",
      "d\n",
      "\n",
      ".\n",
      "\n",
      "f\n",
      "\n",
      "b\n",
      "y\n",
      "g\n",
      "u\n",
      "e\n",
      "s\n",
      "t\n",
      "\n",
      "o\n",
      "n\n",
      "2\n",
      "3\n",
      "D\n",
      "e\n",
      "c\n",
      "e\n",
      "m\n",
      "b\n",
      "e\n",
      "r\n",
      "2\n",
      "0\n",
      "2\n",
      "3\n",
      "\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "\fhttps://doi.org/10.1145/3447548\n",
      ".3467228\n",
      "\n",
      "Julian Eisenschlos, Maharshi Gor, Thomas Mueller,\n",
      "and William Cohen. 2021. MATE: Multi-view\n",
      "attention for table transformer efficiency. In\n",
      "Proceedings of the 2021 Conference on Empir-\n",
      "ical Methods in Natural Language Processing,\n",
      "pages 7606–7619. https://doi.org/10\n",
      ".18653/v1/2021.emnlp-main.600\n",
      "\n",
      "Majid Ghasemi-Gol and Pedro A. Szekely. 2018.\n",
      "TabVec: Table vectors for classification of web\n",
      "tables. CoRR, abs/1802.06290.\n",
      "\n",
      "Michael Glass, Mustafa Canim, Alfio Gliozzo,\n",
      "Saneem Chemmengath, Vishwajeet Kumar,\n",
      "Rishav Chakravarti, Avirup Sil, Feifei Pan,\n",
      "Samarth Bharadwaj, and Nicolas Rodolfo\n",
      "Fauceglia. 2021. Capturing row and column\n",
      "semantics in transformer based question an-\n",
      "swering over tables. In Proceedings of\n",
      "the\n",
      "2021 Conference of the North American Chap-\n",
      "ter of\n",
      "the Association for Computational\n",
      "Linguistics: Human Language Technologies,\n",
      "pages 1212–1224. https://doi.org/10\n",
      ".18653/v1/2021.naacl-main.96\n",
      "\n",
      "Heng Gong, Yawei Sun, Xiaocheng Feng, Bing\n",
      "Qin, Wei Bi, Xiaojiang Liu, and Ting Liu.\n",
      "2020. TableGPT: Few-shot table-to-text gen-\n",
      "eration with table structure reconstruction and\n",
      "content matching. In Proceedings of the 28th In-\n",
      "ternational Conference on Computational Lin-\n",
      "guistics, pages 1978–1988. https://doi.org\n",
      "/10.18653/v1/2020.coling-main.179\n",
      "\n",
      "Yuan Gong, Yu-An Chung, and James Glass.\n",
      "2021. AST: Audio spectrogram transformer.\n",
      "arXiv preprint arXiv:2104.01778v3. https://\n",
      "doi.org/10.21437/Interspeech.2021-698\n",
      "\n",
      "Yury Gorishniy,\n",
      "\n",
      "Ivan Rubachev, Valentin\n",
      "Khrulkov, and Artem Babenko. 2021. Revis-\n",
      "iting deep learning models for tabular data.\n",
      "Advances in Neural Information Processing\n",
      "Systems, 34:18932–18943.\n",
      "\n",
      "Poonam Gupta and Vishal Gupta. 2012. A sur-\n",
      "vey of text question answering techniques.\n",
      "International Journal of Computer Applica-\n",
      "tions, 53(4). https://doi.org/10.5120\n",
      "/8406-2030\n",
      "\n",
      "Jonathan Herzig, Thomas Mueller, Syrine\n",
      "Krichene, and Julian Eisenschlos. 2021. Open\n",
      "domain question answering over tables via\n",
      "dense retrieval. In Proceedings of the 2021\n",
      "Conference of\n",
      "the North American Chap-\n",
      "the Association for Computational\n",
      "ter of\n",
      "Linguistics: Human Language Technologies,\n",
      "512–519. https://doi.org/10\n",
      "pages\n",
      ".18653/v1/2021.naacl-main.43\n",
      "\n",
      "Jonathan Herzig, Pawel Krzysztof Nowak,\n",
      "Thomas Mueller, Francesco Piccinno, and\n",
      "Julian Eisenschlos. 2020. TaPas: Weakly su-\n",
      "In\n",
      "pervised table parsing via pre-training.\n",
      "Proceedings of the 58th Annual Meeting of\n",
      "the Association for Computational Linguistics,\n",
      "pages 4320–4333. https://doi.org/10\n",
      ".18653/v1/2020.acl-main.398\n",
      "\n",
      "Kevin Hu,\n",
      "\n",
      "Snehalkumar’Neil’S Gaikwad,\n",
      "Madelon Hulsebos, Michiel A. Bakker,\n",
      "Emanuel Zgraggen, C´esar Hidalgo, Tim Kraska,\n",
      "Guoliang Li, Arvind Satyanarayan, and C¸ a˘gatay\n",
      "Demiralp. 2019. VizNet: Towards a large-scale\n",
      "visualization learning and benchmarking repos-\n",
      "itory. In Proceedings of the 2019 CHI Confer-\n",
      "ence on Human Factors in Computing Systems,\n",
      "pages 1–12.\n",
      "\n",
      "Kexin Huang,\n",
      "\n",
      "and Rajesh\n",
      "Jaan Altosaar,\n",
      "Ranganath. 2019. ClinicalBERT: Modeling\n",
      "clinical notes and predicting hospital readmis-\n",
      "sion. arXiv preprint arXiv:1904.05342v3.\n",
      "\n",
      "Hiroshi Iida, Dung Thai, Varun Manjunatha, and\n",
      "Mohit Iyyer. 2021. TABBIE: Pretrained rep-\n",
      "resentations of tabular data. In Proceedings of\n",
      "the 2021 Conference of the North American\n",
      "Chapter of the Association for Computational\n",
      "Linguistics: Human Language Technologies,\n",
      "pages 3446–3456. https://doi.org/10\n",
      ".18653/v1/2021.naacl-main.270\n",
      "\n",
      "Sujay Kumar Jauhar, Peter Turney, and Eduard\n",
      "Hovy. 2016. TabMCQ: A dataset of general\n",
      "knowledge tables and multiple-choice ques-\n",
      "tions. arXiv preprint arXiv:1602.03960v1.\n",
      "\n",
      "Georgios Karagiannis, Mohammed Saeed, Paolo\n",
      "Papotti,\n",
      "and Immanuel Trummer. 2020.\n",
      "Scrutinizer: A mixed-initiative approach to\n",
      "large-scale, data-driven claim verification.\n",
      "the VLDB Endowment,\n",
      "Proceedings\n",
      "13(11):2508–2521. https://doi.org/10\n",
      ".14778/3407790.3407841\n",
      "\n",
      "of\n",
      "\n",
      "244\n",
      "\n",
      "l\n",
      "\n",
      "D\n",
      "o\n",
      "w\n",
      "n\n",
      "o\n",
      "a\n",
      "d\n",
      "e\n",
      "d\n",
      "\n",
      "f\n",
      "r\n",
      "o\n",
      "m\n",
      "h\n",
      "\n",
      "t\n",
      "t\n",
      "\n",
      "p\n",
      "\n",
      ":\n",
      "/\n",
      "/\n",
      "\n",
      "d\n",
      "i\n",
      "r\n",
      "e\n",
      "c\n",
      "t\n",
      ".\n",
      "\n",
      "m\n",
      "\n",
      "i\n",
      "t\n",
      ".\n",
      "\n",
      "e\n",
      "d\n",
      "u\n",
      "\n",
      "/\n",
      "t\n",
      "\n",
      "a\n",
      "c\n",
      "l\n",
      "/\n",
      "\n",
      "l\n",
      "\n",
      "a\n",
      "r\n",
      "t\n",
      "i\n",
      "c\n",
      "e\n",
      "-\n",
      "p\n",
      "d\n",
      "\n",
      "f\n",
      "/\n",
      "\n",
      "d\n",
      "o\n",
      "\n",
      "i\n",
      "/\n",
      "\n",
      ".\n",
      "\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "6\n",
      "2\n",
      "\n",
      "/\n",
      "t\n",
      "\n",
      "l\n",
      "\n",
      "a\n",
      "c\n",
      "_\n",
      "a\n",
      "_\n",
      "0\n",
      "0\n",
      "5\n",
      "4\n",
      "4\n",
      "2\n",
      "0\n",
      "7\n",
      "4\n",
      "8\n",
      "7\n",
      "3\n",
      "\n",
      "/\n",
      "\n",
      "/\n",
      "t\n",
      "\n",
      "l\n",
      "\n",
      "a\n",
      "c\n",
      "_\n",
      "a\n",
      "_\n",
      "0\n",
      "0\n",
      "5\n",
      "4\n",
      "4\n",
      "p\n",
      "d\n",
      "\n",
      ".\n",
      "\n",
      "f\n",
      "\n",
      "b\n",
      "y\n",
      "g\n",
      "u\n",
      "e\n",
      "s\n",
      "t\n",
      "\n",
      "o\n",
      "n\n",
      "2\n",
      "3\n",
      "D\n",
      "e\n",
      "c\n",
      "e\n",
      "m\n",
      "b\n",
      "e\n",
      "r\n",
      "2\n",
      "0\n",
      "2\n",
      "3\n",
      "\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "\fMarcin Kaszkiel\n",
      "\n",
      "and Justin Zobel. 1997.\n",
      "SIGIR,\n",
      "Passage\n",
      "pages 178–185. ACM. https://doi.org\n",
      "/10.1145/278459.258561\n",
      "\n",
      "revisited.\n",
      "\n",
      "retrieval\n",
      "\n",
      "In\n",
      "\n",
      "Angelos Katharopoulos, Apoorv Vyas, Nikolaos\n",
      "Pappas, and Franc¸ois Fleuret. 2020. Transform-\n",
      "ers are RNNs: Fast autoregressive transformers\n",
      "with linear attention. In International Confer-\n",
      "ence on Machine Learning, pages 5156–5165.\n",
      "PMLR.\n",
      "\n",
      "George Katsogiannis-Meimarakis and Georgia\n",
      "Koutrika. 2021. A deep dive into deep\n",
      "text-to-SQL sys-\n",
      "learning approaches\n",
      "for\n",
      "tems. In Proceedings of\n",
      "the 2021 Interna-\n",
      "tional Conference on Management of Data,\n",
      "pages 2846–2851. https://doi.org/10\n",
      ".1145/3448016.3457543\n",
      "\n",
      "Salman Khan, Muzammal Naseer, Munawar\n",
      "Hayat, Syed Waqas Zamir, Fahad Shahbaz\n",
      "Khan, and Mubarak Shah. 2021. Transform-\n",
      "in vision: A survey. arXiv preprint\n",
      "ers\n",
      "arXiv:2101.01169.\n",
      "\n",
      "Nikita Kitaev, Lukasz Kaiser, and Anselm\n",
      "Levskaya. 2020. Reformer: The efficient\n",
      "transformer. In International Conference on\n",
      "Learning Representations.\n",
      "\n",
      "Rik Koncel-Kedziorski, Dhanush Bekal, Yi Luan,\n",
      "Mirella Lapata, and Hannaneh Hajishirzi. 2019.\n",
      "Text generation from knowledge graphs with\n",
      "graph transformers. In Proceedings of the 2019\n",
      "Conference of the North American Chapter\n",
      "of the Association for Computational Linguis-\n",
      "tics: Human Language Technologies, Volume 1\n",
      "(Long and Short Papers), pages 2284–2293.\n",
      "\n",
      "Daniel Z. Korman, Eric Mack, Jacob Jett, and\n",
      "Allen H. Renear. 2018. Defining textual entail-\n",
      "ment. Journal of the Association for Informa-\n",
      "tion Science and Technology, 69(6):763–772.\n",
      "https://doi.org/10.1002/asi.24007\n",
      "\n",
      "Bogdan Kosti´c, Julian Risch, and Timo M¨oller.\n",
      "2021. Multi-modal retrieval of tables and texts\n",
      "using tri-encoder models. In Proceedings of\n",
      "the 3rd Workshop on Machine Reading for\n",
      "Question Answering, pages 82–91, Punta Cana,\n",
      "Dominican Republic. Association for Compu-\n",
      "tational Linguistics. https://doi.org/10\n",
      ".18653/v1/2021.mrqa-1.8\n",
      "\n",
      "245\n",
      "\n",
      "Tom Kwiatkowski, Jennimaria Palomaki, Olivia\n",
      "Redfield, Michael Collins, Ankur Parikh, Chris\n",
      "Alberti, Danielle Epstein,\n",
      "Illia Polosukhin,\n",
      "Jacob Devlin, Kenton Lee, et al. 2019. Natural\n",
      "questions: A benchmark for question answer-\n",
      "the Associa-\n",
      "ing research. Transactions of\n",
      "tion for Computational Linguistics, 7:453–466.\n",
      "https://doi.org/10.1162/tacl a 00276\n",
      "\n",
      "Zhenzhong Lan, Mingda Chen, Sebastian\n",
      "Goodman, Kevin Gimpel, Piyush Sharma, and\n",
      "Radu Soricut. 2020. ALBERT: A lite BERT for\n",
      "self-supervised learning of language representa-\n",
      "tions. In International Conference on Learning\n",
      "Representations.\n",
      "\n",
      "Oliver Lehmberg, Dominique Ritze, Robert\n",
      "Meusel, and Christian Bizer. 2016. A large\n",
      "public corpus of web tables containing time and\n",
      "context metadata. In Proceedings of the 25th\n",
      "International Conference Companion on World\n",
      "Wide Web, pages 75–76. https://doi.org\n",
      "/10.1145/2872518.2889386\n",
      "\n",
      "Junyi Li, Tianyi Tang, Wayne Xin Zhao,\n",
      "Zhicheng Wei, Nicholas Jing Yuan, and Ji-Rong\n",
      "Wen. 2021. Few-shot knowledge graph-to-text\n",
      "generation with pretrained language models.\n",
      "In Findings of\n",
      "the Association for Com-\n",
      "putational Linguistics: ACL-IJCNLP 2021,\n",
      "pages 1558–1568.\n",
      "\n",
      "Yuliang Li,\n",
      "\n",
      "Jinfeng Li, Yoshihiko Suhara,\n",
      "AnHai Doan, and Wang-Chiew Tan. 2020.\n",
      "Deep entity matching with pre-trained language\n",
      "models. Proceedings of\n",
      "the VLDB Endow-\n",
      "ment, 14(1):50–60. https://doi.org/10\n",
      ".14778/3421424.3421431\n",
      "\n",
      "Qian Liu, Bei Chen, Jiaqi Guo, Zeqi Lin,\n",
      "and Jian-guang Lou. 2021a. TAPEX: Table\n",
      "pre-training via learning a neural SQL executor.\n",
      "arXiv preprint arXiv:2107.07653v3.\n",
      "\n",
      "Ruibo Liu, Chenyan Jia, Jason Wei, Guangxuan\n",
      "Xu, Lili Wang, and Soroush Vosoughi. 2021b.\n",
      "Mitigating political bias in language models\n",
      "through reinforced calibration. In Thirty-Fifth\n",
      "AAAI Conference on Artificial Intelligence,\n",
      "AAAI, pages 14857–14866. AAAI Press.\n",
      "https://doi.org/10.1609/aaai.v35i17\n",
      ".17744\n",
      "\n",
      "Yinhan Liu, Myle Ott, Naman Goyal, Jingfei\n",
      "Du, Mandar Joshi, Danqi Chen, Omer Levy,\n",
      "Mike Lewis, Luke Zettlemoyer, and Veselin\n",
      "\n",
      "l\n",
      "\n",
      "D\n",
      "o\n",
      "w\n",
      "n\n",
      "o\n",
      "a\n",
      "d\n",
      "e\n",
      "d\n",
      "\n",
      "f\n",
      "r\n",
      "o\n",
      "m\n",
      "h\n",
      "\n",
      "t\n",
      "t\n",
      "\n",
      "p\n",
      "\n",
      ":\n",
      "/\n",
      "/\n",
      "\n",
      "d\n",
      "i\n",
      "r\n",
      "e\n",
      "c\n",
      "t\n",
      ".\n",
      "\n",
      "m\n",
      "\n",
      "i\n",
      "t\n",
      ".\n",
      "\n",
      "e\n",
      "d\n",
      "u\n",
      "\n",
      "/\n",
      "t\n",
      "\n",
      "a\n",
      "c\n",
      "l\n",
      "/\n",
      "\n",
      "l\n",
      "\n",
      "a\n",
      "r\n",
      "t\n",
      "i\n",
      "c\n",
      "e\n",
      "-\n",
      "p\n",
      "d\n",
      "\n",
      "f\n",
      "/\n",
      "\n",
      "d\n",
      "o\n",
      "\n",
      "i\n",
      "/\n",
      "\n",
      ".\n",
      "\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "6\n",
      "2\n",
      "\n",
      "/\n",
      "t\n",
      "\n",
      "l\n",
      "\n",
      "a\n",
      "c\n",
      "_\n",
      "a\n",
      "_\n",
      "0\n",
      "0\n",
      "5\n",
      "4\n",
      "4\n",
      "2\n",
      "0\n",
      "7\n",
      "4\n",
      "8\n",
      "7\n",
      "3\n",
      "\n",
      "/\n",
      "\n",
      "/\n",
      "t\n",
      "\n",
      "l\n",
      "\n",
      "a\n",
      "c\n",
      "_\n",
      "a\n",
      "_\n",
      "0\n",
      "0\n",
      "5\n",
      "4\n",
      "4\n",
      "p\n",
      "d\n",
      "\n",
      ".\n",
      "\n",
      "f\n",
      "\n",
      "b\n",
      "y\n",
      "g\n",
      "u\n",
      "e\n",
      "s\n",
      "t\n",
      "\n",
      "o\n",
      "n\n",
      "2\n",
      "3\n",
      "D\n",
      "e\n",
      "c\n",
      "e\n",
      "m\n",
      "b\n",
      "e\n",
      "r\n",
      "2\n",
      "0\n",
      "2\n",
      "3\n",
      "\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "\fStoyanov. 2019. RoBERTa: A robustly op-\n",
      "timized BERT pretraining approach. CoRR,\n",
      "abs/1907.11692.\n",
      "\n",
      "Moin Nadeem, Anna Bethke, and Siva Reddy.\n",
      "2021. StereoSet: Measuring stereotypical bias\n",
      "in pretrained language models. In Proceedings\n",
      "of the 59th Annual Meeting of the Associa-\n",
      "tion for Computational Linguistics and the 11th\n",
      "International Joint Conference on Natural Lan-\n",
      "guage Processing (Volume 1: Long Papers),\n",
      "pages 5356–5371. https://doi.org/10\n",
      ".18653/v1/2021.acl-long.416\n",
      "\n",
      "for\n",
      "\n",
      "assisting\n",
      "In Proceedings of\n",
      "\n",
      "Preslav Nakov, David Corney, Maram Hasanain,\n",
      "Firoj Alam, Tamer Elsayed, Alberto Barr´on-\n",
      "Cede˜no, Paolo Papotti, Shaden Shaar, and\n",
      "Giovanni Da San Martino. 2021. Automated\n",
      "human\n",
      "fact-checking\n",
      "fact-\n",
      "the Thirtieth\n",
      "checkers.\n",
      "International Joint Conference on Artificial\n",
      "IJCAI-21, pages 4551–4558.\n",
      "Intelligence,\n",
      "International Joint Conferences on Artificial\n",
      "Intelligence Organization. Survey Track.\n",
      "https://doi.org/10.24963/ijcai.2021\n",
      "/619\n",
      "\n",
      "J. Neeraja, Vivek Gupta, and Vivek Srikumar.\n",
      "2021. Incorporating external knowledge to en-\n",
      "hance tabular reasoning. In Proceedings of the\n",
      "2021 Conference of the North American Chap-\n",
      "ter of the Association for Computational Lin-\n",
      "guistics, pages 2799–2809, Online. Association\n",
      "for Computational Linguistics. https://doi\n",
      ".org/10.18653/v1/2021.naacl-main.224\n",
      "\n",
      "Feifei Pan, Mustafa Canim, Michael Glass,\n",
      "Alfio Gliozzo, and Peter Fox. 2021. CLTR:\n",
      "An end-to-end, transformer-based system for\n",
      "cell-level table retrieval and table question an-\n",
      "swering. In Proceedings of the 59th Annual\n",
      "Meeting of the Association for Computational\n",
      "Linguistics and the 11th International Joint\n",
      "Conference on Natural Language Process-\n",
      "ing: System Demonstrations, pages 202–209,\n",
      "Online. Association for Computational Linguis-\n",
      "tics. https://doi.org/10.18653/v1\n",
      "/2021.acl-demo.24\n",
      "\n",
      "Ankur\n",
      "\n",
      "Parikh, Xuezhi Wang,\n",
      "\n",
      "Sebastian\n",
      "Gehrmann, Manaal Faruqui, Bhuwan Dhingra,\n",
      "Diyi Yang, and Dipanjan Das. 2020. ToTTo:\n",
      "A controlled table-to-text generation dataset. In\n",
      "Proceedings of the 2020 Conference on Em-\n",
      "pirical Methods in Natural Language Process-\n",
      "\n",
      "ing (EMNLP), pages 1173–1186. https://doi\n",
      ".org/10.18653/v1/2020.emnlp-main.89\n",
      "\n",
      "Panupong Pasupat and Percy Liang. 2015. Com-\n",
      "positional semantic parsing on semi-structured\n",
      "the 53rd Annual\n",
      "tables. In Proceedings of\n",
      "Meeting of the Association for Computational\n",
      "Linguistics and the 7th International Joint\n",
      "Conference on Natural Language Processing\n",
      "(Volume 1: Long Papers), pages 1470–1480,\n",
      "Beijing, China. Association for Computational\n",
      "Linguistics. https://doi.org/10.3115\n",
      "/v1/P15-1142\n",
      "\n",
      "Alec Radford, Jeffrey Wu, Rewon Child, David\n",
      "Luan, Dario Amodei, and Ilya Sutskever. 2019.\n",
      "Language models are unsupervised multitask\n",
      "learners. OpenAI blog, 1(8):9.\n",
      "\n",
      "Colin Raffel, Noam Shazeer, Adam Roberts,\n",
      "Katherine Lee, Sharan Narang, Michael\n",
      "Matena, Yanqi Zhou, Wei Li, and Peter J. Liu.\n",
      "2020. Exploring the limits of transfer learning\n",
      "with a unified text-to-text transformer. Journal\n",
      "of Machine Learning Research, 21:1–67.\n",
      "\n",
      "Wullianallur Raghupathi and Viju Raghupathi.\n",
      "2014. Big data analytics in healthcare: Promise\n",
      "and potential. Health Information Science and\n",
      "2(1):1–10. https://doi.org\n",
      "Systems,\n",
      "/10.1186/2047-2501-2-3,\n",
      "PubMed:\n",
      "25825667\n",
      "\n",
      "Marco Tulio Ribeiro, Tongshuang Wu, Carlos\n",
      "Guestrin, and Sameer Singh. 2020. Beyond\n",
      "testing of NLP mod-\n",
      "accuracy: Behavioral\n",
      "els with CheckList. In Proceedings of\n",
      "the\n",
      "58th Annual Meeting of the Association for\n",
      "Computational Linguistics, pages 4902–4912,\n",
      "Online. Association for Computational Linguis-\n",
      "tics. https://doi.org/10.18653/v1\n",
      "/2020.acl-main.442\n",
      "\n",
      "Stephen E. Robertson, Steve Walker, Susan Jones,\n",
      "Micheline M. Hancock-Beaulieu, and Mike\n",
      "Gatford. 1995. Okapi at TREC-3. Nist Special\n",
      "Publication Sp, 109:109.\n",
      "\n",
      "Roy Schwartz, Jesse Dodge, Noah A. Smith, and\n",
      "Oren Etzioni. 2020. Green AI. Communications\n",
      "of the ACM, 63(12):54–63. https://doi\n",
      ".org/10.1145/3381831\n",
      "\n",
      "246\n",
      "\n",
      "l\n",
      "\n",
      "D\n",
      "o\n",
      "w\n",
      "n\n",
      "o\n",
      "a\n",
      "d\n",
      "e\n",
      "d\n",
      "\n",
      "f\n",
      "r\n",
      "o\n",
      "m\n",
      "h\n",
      "\n",
      "t\n",
      "t\n",
      "\n",
      "p\n",
      "\n",
      ":\n",
      "/\n",
      "/\n",
      "\n",
      "d\n",
      "i\n",
      "r\n",
      "e\n",
      "c\n",
      "t\n",
      ".\n",
      "\n",
      "m\n",
      "\n",
      "i\n",
      "t\n",
      ".\n",
      "\n",
      "e\n",
      "d\n",
      "u\n",
      "\n",
      "/\n",
      "t\n",
      "\n",
      "a\n",
      "c\n",
      "l\n",
      "/\n",
      "\n",
      "l\n",
      "\n",
      "a\n",
      "r\n",
      "t\n",
      "i\n",
      "c\n",
      "e\n",
      "-\n",
      "p\n",
      "d\n",
      "\n",
      "f\n",
      "/\n",
      "\n",
      "d\n",
      "o\n",
      "\n",
      "i\n",
      "/\n",
      "\n",
      ".\n",
      "\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "6\n",
      "2\n",
      "\n",
      "/\n",
      "t\n",
      "\n",
      "l\n",
      "\n",
      "a\n",
      "c\n",
      "_\n",
      "a\n",
      "_\n",
      "0\n",
      "0\n",
      "5\n",
      "4\n",
      "4\n",
      "2\n",
      "0\n",
      "7\n",
      "4\n",
      "8\n",
      "7\n",
      "3\n",
      "\n",
      "/\n",
      "\n",
      "/\n",
      "t\n",
      "\n",
      "l\n",
      "\n",
      "a\n",
      "c\n",
      "_\n",
      "a\n",
      "_\n",
      "0\n",
      "0\n",
      "5\n",
      "4\n",
      "4\n",
      "p\n",
      "d\n",
      "\n",
      ".\n",
      "\n",
      "f\n",
      "\n",
      "b\n",
      "y\n",
      "g\n",
      "u\n",
      "e\n",
      "s\n",
      "t\n",
      "\n",
      "o\n",
      "n\n",
      "2\n",
      "3\n",
      "D\n",
      "e\n",
      "c\n",
      "e\n",
      "m\n",
      "b\n",
      "e\n",
      "r\n",
      "2\n",
      "0\n",
      "2\n",
      "3\n",
      "\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "\fSofia Serrano and Noah A. Smith. 2019. Is at-\n",
      "tention interpretable? In Proceedings of the\n",
      "57th Annual Meeting of the Association for\n",
      "Computational Linguistics, pages 2931–2951,\n",
      "Florence,\n",
      "Italy. Association for Computa-\n",
      "tional Linguistics. https://doi.org/10\n",
      ".18653/v1/P19-1282\n",
      "\n",
      "Ilya Sutskever, Oriol Vinyals, and Quoc V. Le.\n",
      "2014. Sequence to sequence learning with\n",
      "neural networks. In Proceedings of the 27th\n",
      "International Conference on Neural Informa-\n",
      "tion Processing Systems - Volume 2, NIPS’14,\n",
      "pages 3104–3112, Cambridge, MA, USA.\n",
      "MIT Press.\n",
      "\n",
      "Ravid Shwartz-Ziv and Amitai Armon. 2021.\n",
      "Tabular data: Deep learning is not all you need.\n",
      "In 8th ICML Workshop on Automated Machine\n",
      "Learning (AutoML). https://doi.org\n",
      "/10.1016/j.inffus.2021.11.011\n",
      "\n",
      "Gowthami Somepalli, Micah Goldblum, Avi\n",
      "Schwarzschild, C. Bayan Bruss, and Tom\n",
      "Improved neural\n",
      "Goldstein. 2021. SAINT:\n",
      "networks for tabular data via row attention\n",
      "and contrastive pre-training. arXiv preprint\n",
      "arXiv:2106.01342v1.\n",
      "\n",
      "Emma Strubell, Ananya Ganesh, and Andrew\n",
      "McCallum. 2020. Energy and policy considera-\n",
      "tions for modern deep learning research. In The\n",
      "Thirty-Fourth AAAI Conference on Artificial\n",
      "Intelligence, pages 13693–13696. AAAI Press.\n",
      "https://doi.org/10.1609/aaai.v34i09\n",
      ".7123\n",
      "\n",
      "Lya Hulliyyatus Suadaa, Hidetaka Kamigaito,\n",
      "Kotaro Funakoshi, Manabu Okumura, and\n",
      "Hiroya Takamura. 2021. Towards table-to-text\n",
      "generation with numerical reasoning. In Pro-\n",
      "ceedings of the 59th Annual Meeting of the\n",
      "Association for Computational Linguistics and\n",
      "the 11th International Joint Conference on Nat-\n",
      "ural Language Processing (Volume 1: Long\n",
      "Papers), pages 1451–1465.\n",
      "\n",
      "Yoshihiko Suhara,\n",
      "\n",
      "Jinfeng Li, Yuliang Li,\n",
      "Dan Zhang, C¸ a˘gatay Demiralp, Chen Chen,\n",
      "and Wang-Chiew Tan. 2022. Annotating\n",
      "columns with pre-trained language mod-\n",
      "the 2022 Interna-\n",
      "els.\n",
      "tional Conference on Management of Data,\n",
      "pages 1493–1503. https://doi.org/10\n",
      ".1145/3514221.3517906\n",
      "\n",
      "In Proceedings of\n",
      "\n",
      "Yibo Sun, Zhao Yan, Duyu Tang, Nan Duan,\n",
      "and Bing Qin. 2019. Content-based table\n",
      "for web queries. Neurocomput-\n",
      "retrieval\n",
      "ing, 349:183–189. https://doi.org/10\n",
      ".1016/j.neucom.2018.10.033\n",
      "\n",
      "247\n",
      "\n",
      "Nan Tang, Ju Fan, Fangyi Li, Jianhong Tu,\n",
      "Xiaoyong Du, Guoliang Li, Samuel Madden,\n",
      "and Mourad Ouzzani. 2021. RPT: Rela-\n",
      "tional pre-trained transformer is almost all\n",
      "you need towards democratizing data prepa-\n",
      "ration. Proceedings of the VLDB Endowment,\n",
      "14(8):1254–1261. https://doi.org/10\n",
      ".14778/3457390.3457391\n",
      "\n",
      "Yi Tay, Mostafa Dehghani, Dara Bahri, and\n",
      "Donald Metzler. 2020. Efficient transformers:\n",
      "A survey. CoRR, abs/2009.06732.\n",
      "\n",
      "James Thorne, Andreas Vlachos, Christos\n",
      "Christodoulopoulos, and Arpit Mittal. 2018.\n",
      "FEVER: A large-scale dataset for fact ex-\n",
      "traction and verification. In Conference of the\n",
      "North American Chapter of the Association for\n",
      "Computational Linguistics: Human Language\n",
      "Technologies, NAACL-HLT, pages 809–819.\n",
      "ACL. https://doi.org/10.18653/v1\n",
      "/N18-1074\n",
      "\n",
      "James Thorne, Majid Yazdani, Marzieh Saeidi,\n",
      "Fabrizio Silvestri, Sebastian Riedel, and Alon\n",
      "Halevy. 2021. Database reasoning over text.\n",
      "In Proceedings of\n",
      "the 59th Annual Meet-\n",
      "the Association for Computational\n",
      "ing of\n",
      "Linguistics and the 11th International Joint\n",
      "Conference on Natural Language Processing\n",
      "(Volume 1: Long Papers), pages 3091–3104,\n",
      "Online. Association for Computational Lin-\n",
      "guistics. https://doi.org/10.18653\n",
      "/v1/2021.acl-long.241\n",
      "\n",
      "Roger P. G. Van Gompel and Martin J. Pickering.\n",
      "2007. Syntactic parsing. In The Oxford hand-\n",
      "book of psycholinguistics, pages 289–307.\n",
      "https://doi.org/10.1093/oxfordhb\n",
      "/9780198568971.013.0017\n",
      "\n",
      "Ashish Vaswani, Noam Shazeer, Niki Parmar,\n",
      "Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,\n",
      "Łukasz Kaiser, and Illia Polosukhin. 2017. At-\n",
      "tention is all you need. In Advances in Neural\n",
      "\n",
      "l\n",
      "\n",
      "D\n",
      "o\n",
      "w\n",
      "n\n",
      "o\n",
      "a\n",
      "d\n",
      "e\n",
      "d\n",
      "\n",
      "f\n",
      "r\n",
      "o\n",
      "m\n",
      "h\n",
      "\n",
      "t\n",
      "t\n",
      "\n",
      "p\n",
      "\n",
      ":\n",
      "/\n",
      "/\n",
      "\n",
      "d\n",
      "i\n",
      "r\n",
      "e\n",
      "c\n",
      "t\n",
      ".\n",
      "\n",
      "m\n",
      "\n",
      "i\n",
      "t\n",
      ".\n",
      "\n",
      "e\n",
      "d\n",
      "u\n",
      "\n",
      "/\n",
      "t\n",
      "\n",
      "a\n",
      "c\n",
      "l\n",
      "/\n",
      "\n",
      "l\n",
      "\n",
      "a\n",
      "r\n",
      "t\n",
      "i\n",
      "c\n",
      "e\n",
      "-\n",
      "p\n",
      "d\n",
      "\n",
      "f\n",
      "/\n",
      "\n",
      "d\n",
      "o\n",
      "\n",
      "i\n",
      "/\n",
      "\n",
      ".\n",
      "\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "6\n",
      "2\n",
      "\n",
      "/\n",
      "t\n",
      "\n",
      "l\n",
      "\n",
      "a\n",
      "c\n",
      "_\n",
      "a\n",
      "_\n",
      "0\n",
      "0\n",
      "5\n",
      "4\n",
      "4\n",
      "2\n",
      "0\n",
      "7\n",
      "4\n",
      "8\n",
      "7\n",
      "3\n",
      "\n",
      "/\n",
      "\n",
      "/\n",
      "t\n",
      "\n",
      "l\n",
      "\n",
      "a\n",
      "c\n",
      "_\n",
      "a\n",
      "_\n",
      "0\n",
      "0\n",
      "5\n",
      "4\n",
      "4\n",
      "p\n",
      "d\n",
      "\n",
      ".\n",
      "\n",
      "f\n",
      "\n",
      "b\n",
      "y\n",
      "g\n",
      "u\n",
      "e\n",
      "s\n",
      "t\n",
      "\n",
      "o\n",
      "n\n",
      "2\n",
      "3\n",
      "D\n",
      "e\n",
      "c\n",
      "e\n",
      "m\n",
      "b\n",
      "e\n",
      "r\n",
      "2\n",
      "0\n",
      "2\n",
      "3\n",
      "\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "\fInformation Processing Systems, volume 30.\n",
      "Curran Associates, Inc.\n",
      "\n",
      "preprint arXiv:2205.09843v1. https://doi\n",
      ".org/10.18653/v1/2022.suki-1.5\n",
      "\n",
      "Enzo Veltri, Gilbert Badaro, Mohammed Saeed,\n",
      "and Paolo Papotti. 2023. Data ambiguity profil-\n",
      "ing for the generation of training examples. In\n",
      "39th IEEE International Conference on Data\n",
      "Engineering, ICDE 2023, Anaheim, California,\n",
      "USA, April 3–7, 2023. IEEE.\n",
      "\n",
      "Enzo Veltri, Donatello Santoro, Gilbert Badaro,\n",
      "Mohammed Saeed, and Paolo Papotti. 2022.\n",
      "Pythia: Unsupervised generation of ambiguous\n",
      "textual claims from relational data. In Pro-\n",
      "ceedings of\n",
      "the 2022 International Confer-\n",
      "ence on Management of Data, SIGMOD ’22,\n",
      "pages 2409–2412, New York, NY, USA. Asso-\n",
      "ciation for Computing Machinery. https://\n",
      "doi.org/10.1145/3514221.3520164\n",
      "\n",
      "Jesse Vig, Sebastian Gehrmann, Yonatan\n",
      "Belinkov, Sharon Qian, Daniel Nevo, Yaron\n",
      "Singer, and Stuart Shieber. 2020. Investigating\n",
      "gender bias in language models using causal\n",
      "mediation analysis. Advances in Neural Infor-\n",
      "mation Processing Systems, 33:12388–12401.\n",
      "\n",
      "Fei Wang, Kexuan Sun, Muhao Chen, Jay Pujara,\n",
      "and Pedro Szekely. 2021a. Retrieving complex\n",
      "tables with multi-granular graph representation\n",
      "learning. In Proceedings of the 44th Interna-\n",
      "tional ACM SIGIR Conference on Research\n",
      "and Development in Information Retrieval, SI-\n",
      "GIR ’21, pages 1472–1482, New York, NY,\n",
      "USA. Association for Computing Machinery.\n",
      "\n",
      "Sinong Wang, Belinda Z. Li, Madian Khabsa,\n",
      "Han Fang, and Hao Ma. 2020. Linformer:\n",
      "Self-attention with linear complexity. arXiv\n",
      "preprint arXiv:2006.04768v3.\n",
      "\n",
      "Zhiruo Wang, Haoyu Dong, Ran Jia, Jia Li,\n",
      "Zhiyi Fu, Shi Han, and Dongmei Zhang.\n",
      "2021b. TUTA: Tree-based transformers for\n",
      "generally structured table pre-training. In Pro-\n",
      "ceedings of the 27th ACM SIGKDD Conference\n",
      "on Knowledge Discovery & Data Mining,\n",
      "pages 1780–1790. https://doi.org/10\n",
      ".1145/3447548.3467434\n",
      "\n",
      "Zhiruo Wang, Zhengbao Jiang, Eric Nyberg, and\n",
      "Graham Neubig. 2022. Table retrieval may not\n",
      "necessitate table-specific model design. arXiv\n",
      "\n",
      "Tianbao Xie, Chen Henry Wu, Peng Shi, Ruiqi\n",
      "Zhong, Torsten Scholak, Michihiro Yasunaga,\n",
      "Chien-Sheng Wu, Ming Zhong, Pengcheng\n",
      "Yin, Sida I. Wang, Victor Zhong, Bailin\n",
      "Wang, Chengzu Li, Connor Boyle, Ansong Ni,\n",
      "Ziyu Yao, Dragomir Radev, Caiming Xiong,\n",
      "Lingpeng Kong, Rui Zhang, Noah A. Smith,\n",
      "Luke Zettlemoyer, and Tao Yu. 2022. Unified-\n",
      "SKG: Unifying and multi-tasking structured\n",
      "knowledge grounding with text-to-text\n",
      "lan-\n",
      "guage models. arXiv preprint arXiv:2201\n",
      ".05966v3.\n",
      "\n",
      "Jingfeng Yang, Aditya Gupta, Shyam Upadhyay,\n",
      "Luheng He, Rahul Goel, and Shachi Paul. 2022.\n",
      "TableFormer: Robust transformer modeling for\n",
      "table-text encoding. In Proceedings of the 60th\n",
      "Annual Meeting of the Association for Com-\n",
      "putational Linguistics (Volume 1: Long Pa-\n",
      "pers), pages 528–537. https://doi.org\n",
      "/10.18653/v1/2022.acl-long.40\n",
      "\n",
      "Xiaoyu Yang and Xiaodan Zhu. 2021. Exploring\n",
      "decomposition for table-based fact verification.\n",
      "In EMNLP, pages 1045–1052, Punta Cana,\n",
      "Dominican Republic. Association for Com-\n",
      "putational Linguistics. https://doi.org/10\n",
      ".18653/v1/2021.findings-emnlp.90\n",
      "\n",
      "Pengcheng Yin, Graham Neubig, Wen-tau Yih,\n",
      "and Sebastian Riedel. 2020. TaBERT: Pretrain-\n",
      "ing for joint understanding of textual and tabular\n",
      "data. In Proceedings of the 58th Annual Meeting\n",
      "of the Association for Computational Linguis-\n",
      "tics, pages 8413–8426, Online. Association for\n",
      "Computational Linguistics.\n",
      "\n",
      "Tao Yu, Chien-Sheng Wu, Xi Victoria Lin,\n",
      "Bailin Wang, Yi Chern Tan, Xinyi Yang,\n",
      "Dragomir Radev, Richard Socher, and Caiming\n",
      "Xiong. 2021. GraPPa: Grammar-augmented\n",
      "pre-training for\n",
      "In\n",
      "International Conference on Learning Repre-\n",
      "sentations.\n",
      "\n",
      "table semantic parsing.\n",
      "\n",
      "Tao Yu, Rui Zhang, Kai Yang, Michihiro\n",
      "Yasunaga, Dongxu Wang, Zifan Li, James\n",
      "Ma, Irene Li, Qingning Yao, Shanelle Roman,\n",
      "Zilin Zhang, and Dragomir Radev. 2018. Spi-\n",
      "der: A large-scale human-labeled dataset for\n",
      "complex and cross-domain semantic parsing\n",
      "\n",
      "248\n",
      "\n",
      "l\n",
      "\n",
      "D\n",
      "o\n",
      "w\n",
      "n\n",
      "o\n",
      "a\n",
      "d\n",
      "e\n",
      "d\n",
      "\n",
      "f\n",
      "r\n",
      "o\n",
      "m\n",
      "h\n",
      "\n",
      "t\n",
      "t\n",
      "\n",
      "p\n",
      "\n",
      ":\n",
      "/\n",
      "/\n",
      "\n",
      "d\n",
      "i\n",
      "r\n",
      "e\n",
      "c\n",
      "t\n",
      ".\n",
      "\n",
      "m\n",
      "\n",
      "i\n",
      "t\n",
      ".\n",
      "\n",
      "e\n",
      "d\n",
      "u\n",
      "\n",
      "/\n",
      "t\n",
      "\n",
      "a\n",
      "c\n",
      "l\n",
      "/\n",
      "\n",
      "l\n",
      "\n",
      "a\n",
      "r\n",
      "t\n",
      "i\n",
      "c\n",
      "e\n",
      "-\n",
      "p\n",
      "d\n",
      "\n",
      "f\n",
      "/\n",
      "\n",
      "d\n",
      "o\n",
      "\n",
      "i\n",
      "/\n",
      "\n",
      ".\n",
      "\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "6\n",
      "2\n",
      "\n",
      "/\n",
      "t\n",
      "\n",
      "l\n",
      "\n",
      "a\n",
      "c\n",
      "_\n",
      "a\n",
      "_\n",
      "0\n",
      "0\n",
      "5\n",
      "4\n",
      "4\n",
      "2\n",
      "0\n",
      "7\n",
      "4\n",
      "8\n",
      "7\n",
      "3\n",
      "\n",
      "/\n",
      "\n",
      "/\n",
      "t\n",
      "\n",
      "l\n",
      "\n",
      "a\n",
      "c\n",
      "_\n",
      "a\n",
      "_\n",
      "0\n",
      "0\n",
      "5\n",
      "4\n",
      "4\n",
      "p\n",
      "d\n",
      "\n",
      ".\n",
      "\n",
      "f\n",
      "\n",
      "b\n",
      "y\n",
      "g\n",
      "u\n",
      "e\n",
      "s\n",
      "t\n",
      "\n",
      "o\n",
      "n\n",
      "2\n",
      "3\n",
      "D\n",
      "e\n",
      "c\n",
      "e\n",
      "m\n",
      "b\n",
      "e\n",
      "r\n",
      "2\n",
      "0\n",
      "2\n",
      "3\n",
      "\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "\fand text-to-SQL task. In Proceedings of the\n",
      "2018 Conference on Empirical Methods in Nat-\n",
      "ural Language Processing, pages 3911–3921,\n",
      "Brussels, Belgium. Association for Computa-\n",
      "tional Linguistics. https://doi.org/10\n",
      ".18653/v1/D18-1425\n",
      "\n",
      "Shuo Zhang and Krisztian Balog. 2017. En-\n",
      "tiTables: Smart assistance for entity-focused\n",
      "the 40th Interna-\n",
      "tables. In Proceedings of\n",
      "\n",
      "tional ACM SIGIR Conference on Research\n",
      "in Information Retrieval,\n",
      "and Development\n",
      "pages 255–264. ACM. https://doi.org\n",
      "/10.1145/3077136.3080796\n",
      "\n",
      "Victor Zhong, Caiming Xiong, and Richard\n",
      "Socher. 2017. Seq2SQL: Generating struc-\n",
      "tured queries\n",
      "language us-\n",
      "from natural\n",
      "learning. arXiv preprint\n",
      "ing reinforcement\n",
      "arXiv:1709.00103v7.\n",
      "\n",
      "l\n",
      "\n",
      "D\n",
      "o\n",
      "w\n",
      "n\n",
      "o\n",
      "a\n",
      "d\n",
      "e\n",
      "d\n",
      "\n",
      "f\n",
      "r\n",
      "o\n",
      "m\n",
      "h\n",
      "\n",
      "t\n",
      "t\n",
      "\n",
      "p\n",
      "\n",
      ":\n",
      "/\n",
      "/\n",
      "\n",
      "d\n",
      "i\n",
      "r\n",
      "e\n",
      "c\n",
      "t\n",
      ".\n",
      "\n",
      "m\n",
      "\n",
      "i\n",
      "t\n",
      ".\n",
      "\n",
      "e\n",
      "d\n",
      "u\n",
      "\n",
      "/\n",
      "t\n",
      "\n",
      "a\n",
      "c\n",
      "l\n",
      "/\n",
      "\n",
      "l\n",
      "\n",
      "a\n",
      "r\n",
      "t\n",
      "i\n",
      "c\n",
      "e\n",
      "-\n",
      "p\n",
      "d\n",
      "\n",
      "f\n",
      "/\n",
      "\n",
      "d\n",
      "o\n",
      "\n",
      "i\n",
      "/\n",
      "\n",
      ".\n",
      "\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "6\n",
      "2\n",
      "\n",
      "/\n",
      "t\n",
      "\n",
      "l\n",
      "\n",
      "a\n",
      "c\n",
      "_\n",
      "a\n",
      "_\n",
      "0\n",
      "0\n",
      "5\n",
      "4\n",
      "4\n",
      "2\n",
      "0\n",
      "7\n",
      "4\n",
      "8\n",
      "7\n",
      "3\n",
      "\n",
      "/\n",
      "\n",
      "/\n",
      "t\n",
      "\n",
      "l\n",
      "\n",
      "a\n",
      "c\n",
      "_\n",
      "a\n",
      "_\n",
      "0\n",
      "0\n",
      "5\n",
      "4\n",
      "4\n",
      "p\n",
      "d\n",
      "\n",
      ".\n",
      "\n",
      "f\n",
      "\n",
      "b\n",
      "y\n",
      "g\n",
      "u\n",
      "e\n",
      "s\n",
      "t\n",
      "\n",
      "o\n",
      "n\n",
      "2\n",
      "3\n",
      "D\n",
      "e\n",
      "c\n",
      "e\n",
      "m\n",
      "b\n",
      "e\n",
      "r\n",
      "2\n",
      "0\n",
      "2\n",
      "3\n",
      "\n",
      "249\n",
      "\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "\f\n"
     ]
    }
   ],
   "source": [
    "from pdfminer.high_level import extract_text\n",
    "\n",
    "text = extract_text(\"survey.pdf\")\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited.\n",
      "* In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place.\n",
      "* We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks.\n",
      "* When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.\n",
      "* Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/essam/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/essam/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from rake_nltk import Rake\n",
    "import string\n",
    "\n",
    "nltk.download('punkt')  # Download the punkt tokenizer data if not already installed\n",
    "nltk.download('stopwords')  # Download the punkt tokenizer data if not already installed\n",
    "\n",
    "rake_nltk_var = Rake()\n",
    "\n",
    "def get_sentences(paragraph):\n",
    "    paragraph = paragraph.replace(\"\\n\", \" \")\n",
    "    sentences = nltk.sent_tokenize(paragraph)\n",
    "    return sentences\n",
    "\n",
    "def get_bullets_from_sents(sentences):\n",
    "    bullets_text = \"\"\n",
    "    for sentence in sentences:\n",
    "        sentence = sentence.strip().lstrip(string.punctuation).strip()\n",
    "        \n",
    "        bullets_text += \"* \" + sentence  + \"\\n\"\n",
    "    return bullets_text\n",
    "\n",
    "def get_keywords(sentence):\n",
    "    rake_nltk_var.extract_keywords_from_text(sentence)\n",
    "    keywords_extracted = rake_nltk_var.get_ranked_phrases()\n",
    "    keywords_extracted.sort(key=lambda x: sentence.lower().find(x))\n",
    "    return keywords_extracted\n",
    "    \n",
    "# Example usage:\n",
    "paragraph = \"\"\" \n",
    "While the Transformer architecture has become the de-facto standard for natural\n",
    "language processing tasks, its applications to computer vision remain limited. In\n",
    "vision, attention is either applied in conjunction with convolutional networks, or\n",
    "used to replace certain components of convolutional networks while keeping their\n",
    "overall structure in place. We show that this reliance on CNNs is not necessary\n",
    "and a pure transformer applied directly to sequences of image patches can perform\n",
    "very well on image classification tasks. When pre-trained on large amounts of\n",
    "data and transferred to multiple mid-sized or small image recognition benchmarks\n",
    "(ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent\n",
    "results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.\n",
    "\"\"\"\n",
    "# remove new lines from paragraph\n",
    "paragraph = paragraph.replace(\"\\n\", \" \")\n",
    "sents = get_sentences(paragraph)\n",
    "\n",
    "print(get_bullets_from_sents(sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['natural language processing tasks', 'computer vision remain limited', 'de-facto', 'transformer architecture', 'applications', 'become']\n",
      "['vision', 'convolutional networks', 'either applied', 'keeping', 'attention', 'used', 'overall structure', 'replace certain components', 'place', 'conjunction']\n",
      "['pure transformer applied directly', 'image patches', 'well', 'image classification tasks', 'cnns', 'perform', 'sequences', 'necessary', 'show', 'reliance']\n",
      "['transferred', 'imagenet', 'data', 'large amounts', 'small image recognition benchmarks', 'pre-trained', 'etc', 'mid-sized', 'vtab', 'cifar', 'CIFAR-100']\n",
      "['attains excellent results compared', 'state-of', 'the-art', 'vit', 'train', 'vision transformer', 'requiring substantially fewer computational resources']\n",
      "- While the <span class='color-4'>Transformer architecture</span> has <span class='color-2'>become</span> the <span class='color-3'>de-facto</span> standard for <span class='color-1'>natural language processing tasks</span>, its <span class='color-1'>applications</span> to <span class='color-2'>computer vision remain limited</span>.\n",
      "- In <span class='color-1'>vision</span>, <span class='color-1'>attention</span> is <span class='color-3'>either applied</span> in <span class='color-2'>conjunction</span> with <span class='color-2'>convolutional networks</span>, or <span class='color-2'>used</span> to <span class='color-4'>re<span class='color-1'>place</span> certain components</span> of <span class='color-2'>convolutional networks</span> while <span class='color-4'>keeping</span> their <span class='color-3'>overall structure</span> in <span class='color-1'>place</span>.\n",
      "- We <span class='color-1'>show</span> that this <span class='color-2'>reliance</span> on <span class='color-1'>CNNs</span> is not <span class='color-4'>necessary</span> and a <span class='color-1'>pure transformer applied directly</span> to <span class='color-3'>sequences</span> of <span class='color-2'>image patches</span> can <span class='color-2'>perform</span> very <span class='color-3'>well</span> on <span class='color-4'>image classification tasks</span>.\n",
      "- When <span class='color-2'>pre-trained</span> on <span class='color-4'>large amounts</span> of <span class='color-3'>data</span> and <span class='color-1'>transferred</span> to multiple <span class='color-4'>mid-sized</span> or <span class='color-1'>small image recognition benchmarks</span> (<span class='color-2'>ImageNet</span>, <span class='color-2'>CIFAR</span>-100, <span class='color-1'>VTAB</span>, <span class='color-3'>etc</span>.\n",
      "- <span class='color-2'>Vision Transformer</span> (<span class='color-4'>ViT</span>) <span class='color-1'>attains excellent results compared</span> to <span class='color-2'>state-of</span>-<span class='color-3'>the-art</span> convolutional networks while <span class='color-3'>requiring substantially fewer computational resources</span> to <span class='color-1'>train</span>.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def get_sentences(paragraph):\n",
    "    paragraph = paragraph.replace(\"\\n\", \" \")\n",
    "    sentences = nltk.sent_tokenize(paragraph)\n",
    "    return sentences\n",
    "\n",
    "def get_bullets_from_sents(sentences):\n",
    "    bullets_text = \"\"\n",
    "    for sentence in sentences:\n",
    "        sentence = colorize(sentence)\n",
    "        bullets_text += \"- \" + sentence + \"\\n\"\n",
    "    return bullets_text\n",
    "\n",
    "def colorize(sentence, classes=[\"color-1\", \"color-2\", \"color-3\", \"color-4\"]):\n",
    "    sentence = sentence.strip().lstrip(string.punctuation).strip()\n",
    "    keywords_extracted = get_keywords(sentence)\n",
    "    print(keywords_extracted)\n",
    "    for i, keyword in enumerate(keywords_extracted):\n",
    "        if keyword.casefold() in sentence.casefold():\n",
    "            pattern = re.compile(re.escape(keyword), flags=re.IGNORECASE)\n",
    "            sentence = re.sub(pattern, lambda match: f\"<span class='{classes[i % 4]}'>{match.group()}</span>\", sentence)\n",
    "    return sentence\n",
    "\n",
    "def get_keywords(sentence):\n",
    "    rake_nltk_var.extract_keywords_from_text(sentence)\n",
    "    keywords_extracted = rake_nltk_var.get_ranked_phrases()\n",
    "    keywords_extracted.sort(key=lambda x: sentence.lower().find(x))\n",
    "    \n",
    "   # Create a pattern to match any word that has a hyphen in it\n",
    "    pattern = re.compile(r\"\\b\\w+-\\w+\\b\")\n",
    "    words_with_hyphen = pattern.findall(sentence)\n",
    "    for i, keyword in enumerate(keywords_extracted):\n",
    "        for word in words_with_hyphen:\n",
    "            for single_keyword in keyword.split(\" \"):\n",
    "                if single_keyword in word:\n",
    "                    keywords_extracted[i] = word\n",
    "    \n",
    "    return list(set(keywords_extracted))\n",
    "\n",
    "\n",
    "sents = get_sentences(paragraph)\n",
    "\n",
    "print(get_bullets_from_sents(sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForNextSentencePrediction: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForNextSentencePrediction from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForNextSentencePrediction from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  1130,  4152,   117,  2209,  1110,  1719,  3666,  1107,  9342,\n",
      "          1114, 14255,  6005, 18404,  1348,  6379,   117,  1137,  1215,  1106,\n",
      "          4971,  2218,  5644,  1104, 14255,  6005, 18404,  1348,  6379,  1229,\n",
      "          3709,  1147,  2905,  2401,  1107,  1282,   102,  1284,  1437,  1115,\n",
      "          1142, 24727,  1113, 13597,  1116,  1110,  1136,  3238,  1105,   170,\n",
      "          5805, 11303,  1200,  3666,  2626,  1106, 10028,  1104,  3077, 14879,\n",
      "          1169,  3870,  1304,  1218,  1113,  3077,  5393,  8249,   119,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "0.9977688789367676\n"
     ]
    }
   ],
   "source": [
    "from torch.nn.functional import softmax\n",
    "from transformers import BertForNextSentencePrediction, BertTokenizer\n",
    "\n",
    "\n",
    "seq_B = 'We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks.'\n",
    "seq_A = 'In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place'\n",
    "\n",
    "# load pretrained model and a pretrained tokenizer\n",
    "model = BertForNextSentencePrediction.from_pretrained('bert-base-cased')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "\n",
    "# encode the two sequences. Particularly, make clear that they must be \n",
    "# encoded as \"one\" input to the model by using 'seq_B' as the 'text_pair'\n",
    "encoded = tokenizer.encode_plus(seq_A, text_pair=seq_B, return_tensors='pt')\n",
    "print(encoded)\n",
    "# {'input_ids': tensor([[  101,   146,  1176, 18621,   106,   102,  2091,  1128,  1176,  1172, 136,   102]]),\n",
    "#  'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1]]),\n",
    "#  'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
    "# NOTE how the token_type_ids are 0 for all tokens in seq_A and 1 for seq_B, \n",
    "# this way the model knows which token belongs to which sequence\n",
    "\n",
    "# a model's output is a tuple, we only need the output tensor containing\n",
    "# the relationships which is the first item in the tuple\n",
    "seq_relationship_logits = model(**encoded)[0]\n",
    "\n",
    "# we still need softmax to convert the logits into probabilities\n",
    "# index 0: sequence B is a continuation of sequence A\n",
    "# index 1: sequence B is a random sequence\n",
    "probs = softmax(seq_relationship_logits, dim=1)\n",
    "\n",
    "print(probs[0][0].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a  sample  string \n",
      " with  backslashes.\n"
     ]
    }
   ],
   "source": [
    "original_string = \"This is a \\ sample \\ string \\n with \\ backslashes.\"\n",
    "\n",
    "# Remove all backslashes\n",
    "modified_string = original_string.replace(\"\\\\\", \"\")\n",
    "\n",
    "print(modified_string)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "M1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
